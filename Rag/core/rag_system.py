#!/usr/bin/env python3
"""
å®Œæ•´çš„RAGç³»ç»Ÿå®ç° - é›†æˆæ£€ç´¢å’Œç”Ÿæˆ
"""

import time
from typing import List, Dict, Optional, Union
from dataclasses import dataclass

from embeddings.sentence_transformers_embeddings import SentenceTransformersEmbeddings
from core.document_loader import DocumentLoader
from core.vector_store import VectorStoreManager
from llm.deepseek_llm import DeepSeekLLM


@dataclass
class RAGConfig:
    """RAGç³»ç»Ÿé…ç½®"""
    # åµŒå…¥æ¨¡å‹é…ç½®
    embedding_model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"

    # å‘é‡å­˜å‚¨é…ç½®
    vector_store_type: str = "chroma"  # chroma, faiss

    # LLMé…ç½®
    llm_provider: str = "deepseek"
    llm_model_name: str = "deepseek-chat"

    # æ£€ç´¢é…ç½®
    retrieval_k: int = 3  # æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
    similarity_threshold: float = 0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼

    # ç”Ÿæˆé…ç½®
    max_tokens: int = 1000
    temperature: float = 0.7


@dataclass
class RAGResult:
    """RAGæŸ¥è¯¢ç»“æœ"""
    query: str
    answer: str
    sources: List[Dict]
    retrieval_time: float
    generation_time: float
    total_time: float
    used_context: bool


class CompleteRAGSystem:
    """å®Œæ•´çš„RAGç³»ç»Ÿ"""

    def __init__(self, config: RAGConfig = None):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ

        Args:
            config: RAGé…ç½®
        """
        self.config = config or RAGConfig()

        # åˆå§‹åŒ–ç»„ä»¶
        self.embeddings = None
        self.vector_store_manager = None
        self.vector_store = None
        self.llm = None
        self.document_loader = None

        self._initialize_components()

    def _initialize_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        print("="*60)
        print("åˆå§‹åŒ–RAGç³»ç»Ÿç»„ä»¶...")
        print("="*60)

        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        print("\n1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        self.embeddings = SentenceTransformersEmbeddings(
            model_name=self.config.embedding_model_name
        )

        # 2. åˆå§‹åŒ–æ–‡æ¡£åŠ è½½å™¨
        print("\n2. åˆå§‹åŒ–æ–‡æ¡£åŠ è½½å™¨...")
        self.document_loader = DocumentLoader()

        # 3. åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨
        print("\n3. åˆå§‹åŒ–å‘é‡å­˜å‚¨...")
        self.vector_store_manager = VectorStoreManager()

        # 4. åˆå§‹åŒ–LLM
        print("\n4. åˆå§‹åŒ–LLM...")
        if self.config.llm_provider.lower() == "deepseek":
            self.llm = DeepSeekLLM(model_name=self.config.llm_model_name)
            # æµ‹è¯•è¿æ¥
            if not self.llm.test_connection():
                print("[è­¦å‘Š] LLMè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œå°†åªè¿›è¡Œæ£€ç´¢")
                self.llm = None
        else:
            print(f"[è­¦å‘Š] ä¸æ”¯æŒçš„LLMæä¾›å•†: {self.config.llm_provider}")
            self.llm = None

        # 5. åŠ è½½æ–‡æ¡£å¹¶åˆ›å»ºå‘é‡å­˜å‚¨
        self._load_documents_and_create_store()

        print("\n[å®Œæˆ] RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

    def _load_documents_and_create_store(self):
        """åŠ è½½æ–‡æ¡£å¹¶åˆ›å»ºå‘é‡å­˜å‚¨"""
        print("\n5. åŠ è½½æ–‡æ¡£å¹¶åˆ›å»ºå‘é‡å­˜å‚¨...")

        # åŠ è½½æ–‡æ¡£
        documents = self.document_loader.load_text_documents()

        if not documents:
            print("[ä¿¡æ¯] æœªæ‰¾åˆ°æ–‡æ¡£ï¼Œåˆ›å»ºæµ‹è¯•æ–‡æ¡£...")
            documents = self.document_loader.create_test_documents()

        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = self.vector_store_manager.create_vector_store(
            documents=documents,
            embeddings=self.embeddings,
            vector_store_type=self.config.vector_store_type
        )

        print(f"[å®Œæˆ] å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

    def query(self, question: str, k: int = None) -> RAGResult:
        """
        æ‰§è¡ŒRAGæŸ¥è¯¢

        Args:
            question: ç”¨æˆ·é—®é¢˜
            k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡

        Returns:
            RAGæŸ¥è¯¢ç»“æœ
        """
        if k is None:
            k = self.config.retrieval_k

        start_time = time.time()

        print(f"\n{'='*60}")
        print(f"RAGæŸ¥è¯¢: {question}")
        print(f"{'='*60}")

        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        print("\n[æ­¥éª¤1] æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
        retrieval_start = time.time()

        # ä½¿ç”¨å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„æœç´¢
        retrieved_docs_with_scores = self.vector_store_manager.search_with_scores(
            question, k=k
        )

        retrieval_end = time.time()
        retrieval_time = retrieval_end - retrieval_start

        print(f"[å®Œæˆ] æ£€ç´¢åˆ° {len(retrieved_docs_with_scores)} ä¸ªç›¸å…³æ–‡æ¡£ (è€—æ—¶: {retrieval_time:.2f}ç§’)")

        # 2. ç”Ÿæˆç­”æ¡ˆ
        answer = "LLMæœªé…ç½®ï¼Œæ— æ³•ç”Ÿæˆç­”æ¡ˆ"
        generation_time = 0
        used_context = False

        if self.llm and retrieved_docs_with_scores:
            print("\n[æ­¥éª¤2] ç”Ÿæˆç­”æ¡ˆ...")
            generation_start = time.time()

            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([
                f"æ–‡æ¡£ç‰‡æ®µ{i+1}:\n{doc.page_content}"
                for i, (doc, score) in enumerate(retrieved_docs_with_scores)
            ])

            # ç”Ÿæˆç­”æ¡ˆ
            answer = self.llm.generate_rag_response(
                question, context,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature
            )

            generation_end = time.time()
            generation_time = generation_end - generation_start
            used_context = True

            print(f"[å®Œæˆ] ç­”æ¡ˆç”Ÿæˆå®Œæˆ (è€—æ—¶: {generation_time:.2f}ç§’)")
        elif not self.llm:
            print("[è·³è¿‡] LLMæœªé…ç½®ï¼Œä»…è¿”å›æ£€ç´¢ç»“æœ")
        else:
            print("[è·³è¿‡] æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")

        # 3. è®¡ç®—æ€»æ—¶é—´
        total_time = time.time() - start_time

        # 4. åˆ›å»ºç»“æœå¯¹è±¡
        result = RAGResult(
            query=question,
            answer=answer,
            sources=[{
                'content': doc.page_content,
                'similarity': score,
                'metadata': doc.metadata
            } for doc, score in retrieved_docs_with_scores],
            retrieval_time=retrieval_time,
            generation_time=generation_time,
            total_time=total_time,
            used_context=used_context
        )

        return result

    def print_result(self, result: RAGResult):
        """
        æ‰“å°RAGæŸ¥è¯¢ç»“æœ

        Args:
            result: RAGæŸ¥è¯¢ç»“æœ
        """
        print(f"\n{'='*60}")
        print("æŸ¥è¯¢ç»“æœ")
        print(f"{'='*60}")

        print(f"\nğŸ“ é—®é¢˜: {result.query}")

        if result.used_context:
            print(f"\nğŸ’¬ ç­”æ¡ˆ: {result.answer}")

        print(f"\nğŸ“š å‚è€ƒæ–‡æ¡£ (å…±{len(result.sources)}ä¸ª):")
        for i, source in enumerate(result.sources, 1):
            print(f"\n  [æ–‡æ¡£{i}] ç›¸ä¼¼åº¦: {source['similarity']:.3f}")
            print(f"  å†…å®¹: {source['content'][:100]}...")

        print(f"\nâ±ï¸ æ€§èƒ½ç»Ÿè®¡:")
        print(f"  - æ£€ç´¢æ—¶é—´: {result.retrieval_time:.2f}ç§’")
        print(f"  - ç”Ÿæˆæ—¶é—´: {result.generation_time:.2f}ç§’")
        print(f"  - æ€»æ—¶é—´: {result.total_time:.2f}ç§’")
        print(f"{'='*60}")

    def batch_query(self, questions: List[str]) -> List[RAGResult]:
        """
        æ‰¹é‡æŸ¥è¯¢

        Args:
            questions: é—®é¢˜åˆ—è¡¨

        Returns:
            RAGæŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        print(f"\n{'='*60}")
        print(f"æ‰¹é‡RAGæŸ¥è¯¢ ({len(questions)}ä¸ªé—®é¢˜)")
        print(f"{'='*60}")

        results = []
        for i, question in enumerate(questions, 1):
            print(f"\nå¤„ç†ç¬¬{i}/{len(questions)}ä¸ªé—®é¢˜...")
            result = self.query(question)
            results.append(result)

        return results

    def get_stats(self) -> Dict[str, Union[int, str]]:
        """
        è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            "embedding_model": self.config.embedding_model_name,
            "vector_store_type": self.config.vector_store_type,
            "llm_provider": self.config.llm_provider if self.llm else "æœªé…ç½®",
            "llm_model": self.config.llm_model_name if self.llm else "æœªé…ç½®",
            "retrieval_k": self.config.retrieval_k
        }

        return stats