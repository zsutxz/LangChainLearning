{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20299de2-ec98-4767-a92e-fe93f93b86dc",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U langchain\n",
    "!pip install langchain langchain-community langchain-core\n",
    "!pip install langchain-openai openai\n",
    "!pip install langchain-deepseek\n",
    "!pip install deepseek-api python-dotenv\n",
    "\n",
    "# Installing the OpenAI integration\n",
    "#!pip install -U langchain-openai\n",
    "# Installing the Anthropic integration\n",
    "# !pip install -U langchain-anthropic\n",
    "\n",
    "!pip show langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76dc057-7bf5-4c26-941c-d05a9f334cc3",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-10-28T12:52:06.538368Z",
     "iopub.status.busy": "2025-10-28T12:52:06.538112Z",
     "iopub.status.idle": "2025-10-28T12:52:07.206742Z",
     "shell.execute_reply": "2025-10-28T12:52:07.206242Z",
     "shell.execute_reply.started": "2025-10-28T12:52:06.538351Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# ËÆæÁΩÆ DeepSeek ÁöÑ API ÂØÜÈí•ÔºàLangChain-OpenAI ‰ªçÁÑ∂‰ºöÊü•Êâæ OPENAI_API_KEYÔºâ\n",
    "# ÊÇ®ÂèØ‰ª•Â∞ùËØïÂ∞Ü DEEPSEEK_API_KEY ÁöÑÂÄºËµãÁªô OPENAI_API_KEY ÁéØÂ¢ÉÂèòÈáè\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"DEEPSEEK_API_KEY\", \"your_deepseek_api_key_here\") # Á°Æ‰øùËøô‰∏™ÂÄºÊòØ DeepSeek ÁöÑ key\n",
    "\n",
    "# ÂÖ≥ÈîÆÔºöÊåáÂÆö DeepSeek ÁöÑ API Âü∫Á°Ä URL\n",
    "DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\", # ‰ΩøÁî® DeepSeek ÁöÑÊ®°ÂûãÂêçÁß∞\n",
    "    openai_api_base=DEEPSEEK_BASE_URL, # ÊåáÂÆö DeepSeek ÁöÑ URL\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1079f592-cec4-4e54-b2fd-5dc50f6f113b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-28T12:52:23.441247Z",
     "iopub.status.busy": "2025-10-28T12:52:23.440957Z",
     "iopub.status.idle": "2025-10-28T12:52:23.448532Z",
     "shell.execute_reply": "2025-10-28T12:52:23.448035Z",
     "shell.execute_reply.started": "2025-10-28T12:52:23.441224Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    # tools=[weather_tool, calculator_tool],\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=model,\n",
    "            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
    "            messages_to_keep=20,  # Keep last 20 messages after summary\n",
    "            summary_prompt=\"Custom prompt for summarization...\",  # Optional\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5757067d-8e0f-4b61-9ecb-811f3e1b43db",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-28T12:57:19.092822Z",
     "iopub.status.busy": "2025-10-28T12:57:19.092581Z",
     "iopub.status.idle": "2025-10-28T12:57:19.101836Z",
     "shell.execute_reply": "2025-10-28T12:57:19.101414Z",
     "shell.execute_reply.started": "2025-10-28T12:57:19.092803Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    # tools=[read_email_tool, send_email_tool],\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                # Require approval, editing, or rejection for sending emails\n",
    "                \"send_email_tool\": {\n",
    "                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n",
    "                },\n",
    "                # Auto-approve reading emails\n",
    "                \"read_email_tool\": False,\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d99c81bb-788b-4393-9280-defff490b587",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-28T12:58:15.843056Z",
     "iopub.status.busy": "2025-10-28T12:58:15.842830Z",
     "iopub.status.idle": "2025-10-28T12:58:20.583846Z",
     "shell.execute_reply": "2025-10-28T12:58:20.583419Z",
     "shell.execute_reply.started": "2025-10-28T12:58:15.843041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='c6c4ff9a-20bc-46f6-9aaa-601f7859df7c'),\n",
       "  AIMessage(content=\"I'm sorry, but I don't have access to your name or any personal information unless you've shared it with me in this conversation. If you'd like, you can tell me your name, and I can use it in our conversation!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 23, 'total_tokens': 72, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 23}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '9758f2b3-369c-41d3-a5af-a3e143d420a1', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--3911581c-bef1-4993-8fdb-a3e7bb1618d0-0', usage_metadata={'input_tokens': 23, 'output_tokens': 49, 'total_tokens': 72, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install langchain_anthropic\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "from langchain_anthropic.middleware import AnthropicPromptCachingMiddleware\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "\n",
    "LONG_PROMPT = \"\"\"\n",
    "Please be a helpful assistant.\n",
    "\n",
    "<Lots more context ...>\n",
    "\"\"\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=LONG_PROMPT,\n",
    "    middleware=[AnthropicPromptCachingMiddleware(ttl=\"5m\")],\n",
    ")\n",
    "\n",
    "# cache store\n",
    "agent.invoke({\"messages\": [HumanMessage(\"Hi, my name is Bob\")]})\n",
    "\n",
    "# cache hit, system prompt is cached\n",
    "agent.invoke({\"messages\": [HumanMessage(\"What's my name?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5b561dd-ccd6-4700-bd67-9cf1129e5bc0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-28T12:59:15.631562Z",
     "iopub.status.busy": "2025-10-28T12:59:15.631323Z",
     "iopub.status.idle": "2025-10-28T12:59:15.639113Z",
     "shell.execute_reply": "2025-10-28T12:59:15.638696Z",
     "shell.execute_reply.started": "2025-10-28T12:59:15.631546Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        ModelCallLimitMiddleware(\n",
    "            thread_limit=10,  # Max 10 calls per thread (across runs)\n",
    "            run_limit=5,  # Max 5 calls per run (single invocation)\n",
    "            exit_behavior=\"end\",  # Or \"error\" to raise exception\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92e88814-46a1-4c52-89da-e33ffb601a33",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-28T13:01:31.853451Z",
     "iopub.status.busy": "2025-10-28T13:01:31.853245Z",
     "iopub.status.idle": "2025-10-28T13:01:31.864356Z",
     "shell.execute_reply": "2025-10-28T13:01:31.863966Z",
     "shell.execute_reply.started": "2025-10-28T13:01:31.853437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ToolCallLimitMiddleware\n",
    "\n",
    "\n",
    "# Limit all tool calls\n",
    "global_limiter = ToolCallLimitMiddleware(thread_limit=20, run_limit=10)\n",
    "\n",
    "# Limit specific tool\n",
    "search_limiter = ToolCallLimitMiddleware(\n",
    "    tool_name=\"search\",\n",
    "    thread_limit=5,\n",
    "    run_limit=3,\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[global_limiter, search_limiter],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13bd3cb5-0310-459e-bc87-bd2a50564afd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T13:02:24.417156Z",
     "iopub.status.busy": "2025-10-28T13:02:24.416922Z",
     "iopub.status.idle": "2025-10-28T13:02:24.421943Z",
     "shell.execute_reply": "2025-10-28T13:02:24.421543Z",
     "shell.execute_reply.started": "2025-10-28T13:02:24.417139Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ModelFallbackMiddleware\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,  # Primary model\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        ModelFallbackMiddleware(\n",
    "            model,  # Try first on error\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d625b2e0-593e-4239-8bc2-bc157193783b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T13:03:22.271128Z",
     "iopub.status.busy": "2025-10-28T13:03:22.270920Z",
     "iopub.status.idle": "2025-10-28T13:03:22.286417Z",
     "shell.execute_reply": "2025-10-28T13:03:22.285967Z",
     "shell.execute_reply.started": "2025-10-28T13:03:22.271114Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import PIIMiddleware\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        # Redact emails in user input\n",
    "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
    "        # Mask credit cards (show last 4 digits)\n",
    "        PIIMiddleware(\"credit_card\", strategy=\"mask\", apply_to_input=True),\n",
    "        # Custom PII type with regex\n",
    "        PIIMiddleware(\n",
    "            \"api_key\",\n",
    "            detector=r\"sk-[a-zA-Z0-9]{32}\",\n",
    "            strategy=\"block\",  # Raise error if detected\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6827d160-8782-4584-b544-1d371ae263ab",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-28T13:05:21.991223Z",
     "iopub.status.busy": "2025-10-28T13:05:21.991008Z",
     "iopub.status.idle": "2025-10-28T13:05:30.235855Z",
     "shell.execute_reply": "2025-10-28T13:05:30.235422Z",
     "shell.execute_reply.started": "2025-10-28T13:05:21.991209Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Help me refactor my codebase', additional_kwargs={}, response_metadata={}, id='ec40e76d-c074-4330-8ff6-344eb1b1ed85'), AIMessage(content=\"I'd be happy to help you refactor your codebase! However, to provide the most effective assistance, I'll need some more specific information about your project:\\n\\n1. **What programming language(s) are you working with?**\\n2. **What type of codebase is it?** (web application, API, library, etc.)\\n3. **What specific refactoring goals do you have?** For example:\\n   - Improving code readability/maintainability\\n   - Optimizing performance\\n   - Reducing technical debt\\n   - Following specific design patterns\\n   - Breaking up large functions/classes\\n   - Improving testability\\n\\n4. **Do you have any particular pain points or areas of concern?**\\n5. **Would you like to share some code snippets or file structures** that you'd like me to focus on?\\n\\nOnce I have these details, I can create a structured refactoring plan and provide specific recommendations tailored to your codebase's needs.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 196, 'prompt_tokens': 1226, 'total_tokens': 1422, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1216}, 'prompt_cache_hit_tokens': 1216, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'bc1ed121-48e9-42a2-8d1d-26aa433512a1', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--bd8cd4ad-ed5e-47eb-a0c8-235632f8c334-0', usage_metadata={'input_tokens': 1226, 'output_tokens': 196, 'total_tokens': 1422, 'input_token_details': {'cache_read': 1216}, 'output_token_details': {}})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import TodoListMiddleware\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[TodoListMiddleware()],\n",
    ")\n",
    "\n",
    "result = agent.invoke({\"messages\": [HumanMessage(\"Help me refactor my codebase\")]})\n",
    "# print(result[\"todos\"])  # Array of todo items with status tracking\n",
    "print(result[\"messages\"])  # Array of todo items with status tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "499b732a-354d-46f4-bdc8-c245d0a2fc71",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-28T13:06:53.198933Z",
     "iopub.status.busy": "2025-10-28T13:06:53.198705Z",
     "iopub.status.idle": "2025-10-28T13:06:53.204133Z",
     "shell.execute_reply": "2025-10-28T13:06:53.203741Z",
     "shell.execute_reply.started": "2025-10-28T13:06:53.198915Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ToolRetryMiddleware\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    # tools=[search_tool, database_tool],\n",
    "    middleware=[\n",
    "        ToolRetryMiddleware(\n",
    "            max_retries=3,  # Retry up to 3 times\n",
    "            backoff_factor=2.0,  # Exponential backoff multiplier\n",
    "            initial_delay=1.0,  # Start with 1 second delay\n",
    "            max_delay=60.0,  # Cap delays at 60 seconds\n",
    "            jitter=True,  # Add random jitter to avoid thundering herd\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f027a7a-3005-4ed7-afd7-01415dbea127",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-28T13:07:56.750985Z",
     "iopub.status.busy": "2025-10-28T13:07:56.750774Z",
     "iopub.status.idle": "2025-10-28T13:07:56.756020Z",
     "shell.execute_reply": "2025-10-28T13:07:56.755630Z",
     "shell.execute_reply.started": "2025-10-28T13:07:56.750970Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import LLMToolEmulator\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    # tools=[get_weather, search_database, send_email],\n",
    "    middleware=[\n",
    "        # Emulate all tools by default\n",
    "        LLMToolEmulator(),\n",
    "\n",
    "        # Or emulate specific tools\n",
    "        # LLMToolEmulator(tools=[\"get_weather\", \"search_database\"]),\n",
    "\n",
    "        # Or use a custom model for emulation\n",
    "        # LLMToolEmulator(model=\"anthropic:claude-3-5-sonnet-latest\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d38260de-e33f-4b45-8216-6675d74b68fe",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-28T13:09:53.203472Z",
     "iopub.status.busy": "2025-10-28T13:09:53.203263Z",
     "iopub.status.idle": "2025-10-28T13:09:53.213163Z",
     "shell.execute_reply": "2025-10-28T13:09:53.212745Z",
     "shell.execute_reply.started": "2025-10-28T13:09:53.203459Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import before_model, after_model, wrap_model_call\n",
    "from langchain.agents.middleware import AgentState, ModelRequest, ModelResponse, dynamic_prompt\n",
    "from langchain.messages import AIMessage\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "# Node-style: logging before model calls\n",
    "@before_model\n",
    "def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    print(f\"About to call model with {len(state['messages'])} messages\")\n",
    "    return None\n",
    "\n",
    "# Node-style: validation after model calls\n",
    "@after_model(can_jump_to=[\"end\"])\n",
    "def validate_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"BLOCKED\" in last_message.content:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n",
    "            \"jump_to\": \"end\"\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Wrap-style: retry logic\n",
    "@wrap_model_call\n",
    "def retry_model(\n",
    "    request: ModelRequest,\n",
    "    handler: Callable[[ModelRequest], ModelResponse],\n",
    ") -> ModelResponse:\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            return handler(request)\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n",
    "\n",
    "# Wrap-style: dynamic prompts\n",
    "@dynamic_prompt\n",
    "def personalized_prompt(request: ModelRequest) -> str:\n",
    "    user_id = request.runtime.context.get(\"user_id\", \"guest\")\n",
    "    return f\"You are a helpful assistant for user {user_id}. Be concise and friendly.\"\n",
    "\n",
    "# Use decorators in agent\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    middleware=[log_before_model, validate_output, retry_model, personalized_prompt],\n",
    "    tools=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a67a96a9-ee39-4c92-afef-45953c551529",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-28T13:11:59.228713Z",
     "iopub.status.busy": "2025-10-28T13:11:59.228466Z",
     "iopub.status.idle": "2025-10-28T13:12:00.599045Z",
     "shell.execute_reply": "2025-10-28T13:12:00.598639Z",
     "shell.execute_reply.started": "2025-10-28T13:11:59.228698Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}, id='71ea7815-487f-4d4a-a8c0-29d9e0d71266'),\n",
       "  AIMessage(content='Hello! üëã How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 5, 'total_tokens': 16, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 5}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '4e06935a-6ebb-4e4c-805e-144e1d298e76', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--6f3e5f83-806f-41bc-87df-279a5e6d7056-0', usage_metadata={'input_tokens': 5, 'output_tokens': 11, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})],\n",
       " 'model_call_count': 1,\n",
       " 'user_id': 'user-123'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents.middleware import AgentState, AgentMiddleware\n",
    "from typing_extensions import NotRequired\n",
    "from typing import Any\n",
    "\n",
    "class CustomState(AgentState):\n",
    "    model_call_count: NotRequired[int]\n",
    "    user_id: NotRequired[str]\n",
    "\n",
    "class CallCounterMiddleware(AgentMiddleware[CustomState]):\n",
    "    state_schema = CustomState\n",
    "\n",
    "    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
    "        # Access custom state properties\n",
    "        count = state.get(\"model_call_count\", 0)\n",
    "\n",
    "        if count > 10:\n",
    "            return {\"jump_to\": \"end\"}\n",
    "\n",
    "        return None\n",
    "\n",
    "    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
    "        # Update custom state\n",
    "        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n",
    "    \n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    middleware=[CallCounterMiddleware()],\n",
    "    tools=[],\n",
    ")\n",
    "\n",
    "# Invoke with custom state\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"Hello\")],\n",
    "    \"model_call_count\": 0,\n",
    "    \"user_id\": \"user-123\",\n",
    "})\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
