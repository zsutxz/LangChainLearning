{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20299de2-ec98-4767-a92e-fe93f93b86dc",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T12:12:36.920997Z",
     "iopub.status.busy": "2025-11-04T12:12:36.920799Z",
     "iopub.status.idle": "2025-11-04T12:12:58.077170Z",
     "shell.execute_reply": "2025-11-04T12:12:58.076669Z",
     "shell.execute_reply.started": "2025-11-04T12:12:36.920982Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Collecting langgraph\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/d7/b1/9f4912e13d4ed691f2685c8a4b764b5a9237a30cca0c5782bc213d9f0a9a/langgraph-1.0.2-py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-core>=0.1 (from langgraph)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/f2/1b/b0a37674bdcbd2931944e12ea742fd167098de5212ee2391e91dce631162/langchain_core-1.0.3-py3-none-any.whl (469 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.9/469.9 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/85/2a/2efe0b5a72c41e3a936c81c5f5d8693987a1b260287ff1bbebaae1b7b888/langgraph_checkpoint-3.0.0-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/27/2f/9a7d00d4afa036e65294059c7c912002fb72ba5dbbd5c2a871ca06360278/langgraph_prebuilt-1.0.2-py3-none-any.whl (34 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/66/05/b2d34e16638241e6f27a6946d28160d4b8b641383787646d41a3727e0896/langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/site-packages (from langgraph) (2.11.10)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/site-packages (from langgraph) (3.6.0)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core>=0.1->langgraph)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core>=0.1->langgraph)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/b3/8a/de38127895f185f677778930478c6884b087c2cab5744dd7e27c0188fbec/langsmith-0.4.40-py3-none-any.whl (399 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.3/399.3 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/1c/fa/e7e06835bfea9adeef43915143ce818098aecab0cbd3df584815adf3e399/ormsgpack-1.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/3f/51/d4db610ef29373b879047326cbf6fa98b6c1969d6f6dc423279de2b1be2c/requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: ormsgpack, jsonpatch, requests-toolbelt, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
      "Successfully installed jsonpatch-1.33 langchain-core-1.0.3 langgraph-1.0.2 langgraph-checkpoint-3.0.0 langgraph-prebuilt-1.0.2 langgraph-sdk-0.2.9 langsmith-0.4.40 ormsgpack-1.11.0 requests-toolbelt-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Collecting langchain\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/68/c8/b5dcfdde8b96369e5445f0fbac52fe8495bbd11b23ca83691d90d464eb15/langchain-1.0.3-py3-none-any.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.11/site-packages (from langchain) (1.0.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/site-packages (from langchain) (2.11.10)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (0.4.40)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.11.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: langchain\n",
      "Successfully installed langchain-1.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Collecting langchain-openai\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/78/9b/7af1d539a051d195c5ecc5990ebd483f208c40f75a8a9532846d16762704/langchain_openai-1.0.2-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: openai in /usr/local/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.2 in /usr/local/lib/python3.11/site-packages (from langchain-openai) (1.0.3)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/site-packages (from openai) (2.11.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.4.40)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.3.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: langchain-openai\n",
      "Successfully installed langchain-openai-1.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Collecting langchain-deepseek\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/4f/35/c84672001fd9899f8e40f99f81c81235d2093386ce3d159111002edd8365/langchain_deepseek-1.0.0-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from langchain-deepseek) (1.0.3)\n",
      "Requirement already satisfied: langchain-openai<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from langchain-deepseek) (1.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (0.4.40)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (2.11.10)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (4.15.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.11/site-packages (from langchain-openai<2.0.0,>=1.0.0->langchain-deepseek) (2.3.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/site-packages (from langchain-openai<2.0.0,>=1.0.0->langchain-deepseek) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (0.25.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.0->langchain-deepseek) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.0->langchain-deepseek) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.0->langchain-deepseek) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.0->langchain-deepseek) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.0->langchain-deepseek) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai<2.0.0,>=1.0.0->langchain-deepseek) (2024.11.6)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.0->langchain-deepseek) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-deepseek) (2.3.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: langchain-deepseek\n",
      "Successfully installed langchain-deepseek-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv add langgraph\n",
    "\n",
    "!pip install -U langgraph\n",
    "!pip install -U langchain\n",
    "!pip install langchain-openai openai\n",
    "!pip install langchain-deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f76dc057-7bf5-4c26-941c-d05a9f334cc3",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T13:51:17.033094Z",
     "iopub.status.busy": "2025-11-04T13:51:17.032876Z",
     "iopub.status.idle": "2025-11-04T13:51:17.036795Z",
     "shell.execute_reply": "2025-11-04T13:51:17.036364Z",
     "shell.execute_reply.started": "2025-11-04T13:51:17.033080Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# 设置 DeepSeek 的 API 密钥（LangChain-OpenAI 仍然会查找 OPENAI_API_KEY）\n",
    "os.environ['DEEPSEEK_API_KEY'] = 'sk-943df854319e423ca178e68e4668ca5a'\n",
    "\n",
    "# 您可以尝试将 DEEPSEEK_API_KEY 的值赋给 OPENAI_API_KEY 环境变量\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('DEEPSEEK_API_KEY') # 确保这个值是 DeepSeek 的 key\n",
    "\n",
    "# 关键：指定 DeepSeek 的 API 基础 URL\n",
    "DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\", # 使用 DeepSeek 的模型名称\n",
    "    openai_api_base=DEEPSEEK_BASE_URL, # 指定 DeepSeek 的 URL\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7476e649-433f-4d6a-bec9-4665dd48c7b8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T13:58:07.237176Z",
     "iopub.status.busy": "2025-11-04T13:58:07.236972Z",
     "iopub.status.idle": "2025-11-04T13:58:07.244158Z",
     "shell.execute_reply": "2025-11-04T13:58:07.243720Z",
     "shell.execute_reply.started": "2025-11-04T13:58:07.237162Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'user', 'content': 'hi! i am Bob'}, {'role': 'assistant', 'content': 'Hello!'}]}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# 1. 定义状态\n",
    "class State(TypedDict):\n",
    "    messages: list[dict]\n",
    "\n",
    "# 2. 定义节点函数\n",
    "def echo_node(state: State):\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": \"Hello!\"}]}\n",
    "\n",
    "# 3. 构建图\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"echo\", echo_node)\n",
    "builder.add_edge(START, \"echo\")  # ✅ 必须添加从 START 的边\n",
    "builder.add_edge(\"echo\", END)\n",
    "\n",
    "# 4. 编译并调用\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "result = graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! i am Bob\"}]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de949393-7a48-4f27-8e7d-7abb7e68f042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78a81649-5992-48e5-b28c-bd57499307e2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T13:55:30.250656Z",
     "iopub.status.busy": "2025-11-04T13:55:30.250395Z",
     "iopub.status.idle": "2025-11-04T13:55:30.255368Z",
     "shell.execute_reply": "2025-11-04T13:55:30.254984Z",
     "shell.execute_reply.started": "2025-11-04T13:55:30.250638Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "# Subgraph\n",
    "\n",
    "def subgraph_node_1(state: State):\n",
    "    return {\"foo\": state[\"foo\"] + \"bar\"}\n",
    "\n",
    "subgraph_builder = StateGraph(State)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph = subgraph_builder.compile()  \n",
    "\n",
    "# Parent graph\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node_1\", subgraph)  \n",
    "builder.add_edge(START, \"node_1\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67b59d1a-f953-461f-945b-e8395c3de215",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T14:05:11.011382Z",
     "iopub.status.busy": "2025-11-04T14:05:11.011154Z",
     "iopub.status.idle": "2025-11-04T14:05:18.741033Z",
     "shell.execute_reply": "2025-11-04T14:05:18.740487Z",
     "shell.execute_reply.started": "2025-11-04T14:05:11.011367Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/site-packages (1.0.2)\n",
      "Collecting langgraph-checkpoint-redis\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/c5/93/c72641f948e5057113144ecd4c14ee6394562b57c65babf2b5db8002d103/langgraph_checkpoint_redis-0.1.2-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/site-packages (from langgraph) (1.0.3)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.11/site-packages (from langgraph) (3.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.11/site-packages (from langgraph) (1.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.11/site-packages (from langgraph) (0.2.9)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/site-packages (from langgraph) (2.11.10)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/site-packages (from langgraph) (3.6.0)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/c4/f2/06bf5addf8ee664291e1b9ffa1f28fc9d97e59806dc7de5aea9844cbf335/langgraph_checkpoint-2.1.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: orjson<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/site-packages (from langgraph-checkpoint-redis) (3.11.3)\n",
      "Collecting redis<7.0.0,>=5.2.1 (from langgraph-checkpoint-redis)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/e8/02/89e2ed7e85db6c93dfa9e8f691c5087df4e3551ab39081a4d7c6d1f90e05/redis-6.4.0-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting redisvl<1.0.0,>=0.5.1 (from langgraph-checkpoint-redis)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/a5/15/0cb1a430fabd330c16fb6a16287743c769c0d7c674db48daa57efda2269c/redisvl-0.10.0-py3-none-any.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.4/161.4 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (0.4.40)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.11.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
      "Collecting jsonpath-ng>=1.5.0 (from redisvl<1.0.0,>=0.5.1->langgraph-checkpoint-redis)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/35/5a/73ecb3d82f8615f32ccdadeb9356726d6cae3a4bbc840b437ceb95708063/jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/site-packages (from redisvl<1.0.0,>=0.5.1->langgraph-checkpoint-redis) (0.5.3)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in /usr/local/lib/python3.11/site-packages (from redisvl<1.0.0,>=0.5.1->langgraph-checkpoint-redis) (1.26.4)\n",
      "Collecting python-ulid>=3.0.0 (from redisvl<1.0.0,>=0.5.1->langgraph-checkpoint-redis)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/6c/a0/4ed6632b70a52de845df056654162acdebaf97c20e3212c559ac43e7216e/python_ulid-3.1.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Collecting ply (from jsonpath-ng>=1.5.0->redisvl<1.0.0,>=0.5.1->langgraph-checkpoint-redis)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/a3/58/35da89ee790598a0700ea49b2a66594140f44dec458c07e8e3d4979137fc/ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: ply, redis, python-ulid, jsonpath-ng, redisvl, langgraph-checkpoint, langgraph-checkpoint-redis\n",
      "  Attempting uninstall: langgraph-checkpoint\n",
      "    Found existing installation: langgraph-checkpoint 3.0.0\n",
      "    Uninstalling langgraph-checkpoint-3.0.0:\n",
      "      Successfully uninstalled langgraph-checkpoint-3.0.0\n",
      "Successfully installed jsonpath-ng-1.7.0 langgraph-checkpoint-2.1.2 langgraph-checkpoint-redis-0.1.2 ply-3.11 python-ulid-3.1.0 redis-6.4.0 redisvl-0.10.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\n",
    "!pip install -U langgraph langgraph-checkpoint-redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c53f0d4-0fe3-4d61-acac-1a683c139b21",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T14:01:44.758288Z",
     "iopub.status.busy": "2025-11-04T14:01:44.758058Z",
     "iopub.status.idle": "2025-11-04T14:01:44.820814Z",
     "shell.execute_reply": "2025-11-04T14:01:44.820251Z",
     "shell.execute_reply.started": "2025-11-04T14:01:44.758270Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection failed: connection to server at \"127.0.0.1\", port 5442 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\nMultiple connection attempts failed. All failures were:\n- host: 'localhost', port: '5442', hostaddr: '::1': connection failed: connection to server at \"::1\", port 5442 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n- host: 'localhost', port: '5442', hostaddr: '127.0.0.1': connection failed: connection to server at \"127.0.0.1\", port 5442 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpostgres\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PostgresStore\n\u001b[32m      3\u001b[39m DB_URI = \u001b[33m\"\u001b[39m\u001b[33mpostgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPostgresStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_conn_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDB_URI\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mStateGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m)\u001b[49m  \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langgraph/store/postgres/base.py:781\u001b[39m, in \u001b[36mPostgresStore.from_conn_string\u001b[39m\u001b[34m(cls, conn_string, pipeline, pool_config, index, ttl)\u001b[39m\n\u001b[32m    779\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(conn=pool, index=index, ttl=ttl)\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mConnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconn_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepare_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdict_row\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m pipeline:\n\u001b[32m    785\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m conn.pipeline() \u001b[38;5;28;01mas\u001b[39;00m pipe:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/psycopg/connection.py:127\u001b[39m, in \u001b[36mConnection.connect\u001b[39m\u001b[34m(cls, conninfo, autocommit, prepare_threshold, context, row_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m     lines.append(\u001b[33m\"\u001b[39m\u001b[33mMultiple connection attempts failed. All failures were:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    126\u001b[39m     lines.extend((\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m error, descr \u001b[38;5;129;01min\u001b[39;00m conn_errors))\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(last_ex)(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(lines)).with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    130\u001b[39m     pq.version() >= \u001b[32m160000\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m rv.pgconn.used_gssapi\n\u001b[32m    132\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m gssapi_requested(params))\n\u001b[32m    133\u001b[39m ):\n\u001b[32m    134\u001b[39m     warnings.warn(\n\u001b[32m    135\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe connection was obtained using the GSSAPI relying on the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mgssencmode=prefer\u001b[39m\u001b[33m'\u001b[39m\u001b[33m libpq default. In a future psycopg[binary] version this default will be changed to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdisable\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. If you wish to interact with the GSSAPI reliably please set the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mgssencmode\u001b[39m\u001b[33m'\u001b[39m\u001b[33m parameter in the connection string or the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mPGGSSENCMODE\u001b[39m\u001b[33m'\u001b[39m\u001b[33m environment variable to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mprefer\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrequire\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    136\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    137\u001b[39m     )\n",
      "\u001b[31mOperationalError\u001b[39m: connection failed: connection to server at \"127.0.0.1\", port 5442 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\nMultiple connection attempts failed. All failures were:\n- host: 'localhost', port: '5442', hostaddr: '::1': connection failed: connection to server at \"::1\", port 5442 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n- host: 'localhost', port: '5442', hostaddr: '127.0.0.1': connection failed: connection to server at \"127.0.0.1\", port 5442 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?"
     ]
    }
   ],
   "source": [
    "from langgraph.store.postgres import PostgresStore\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "with PostgresStore.from_conn_string(DB_URI) as store:  \n",
    "    builder = StateGraph(...)\n",
    "    graph = builder.compile(store=store)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f778b3cd-07da-422a-9ac2-414740a6c35a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T14:05:52.754970Z",
     "iopub.status.busy": "2025-11-04T14:05:52.754747Z",
     "iopub.status.idle": "2025-11-04T14:05:53.382927Z",
     "shell.execute_reply": "2025-11-04T14:05:53.382251Z",
     "shell.execute_reply.started": "2025-11-04T14:05:52.754955Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Error 111 connecting to localhost:6379. Connection refused.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/connection.py:389\u001b[39m, in \u001b[36mAbstractConnection.connect_check_health\u001b[39m\u001b[34m(self, check_health, retry_socket_connect)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retry_socket_connect:\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/retry.py:105\u001b[39m, in \u001b[36mRetry.call_with_retry\u001b[39m\u001b[34m(self, do, fail)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m._supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/connection.py:390\u001b[39m, in \u001b[36mAbstractConnection.connect_check_health.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retry_socket_connect:\n\u001b[32m    389\u001b[39m     sock = \u001b[38;5;28mself\u001b[39m.retry.call_with_retry(\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m.disconnect(error)\n\u001b[32m    391\u001b[39m     )\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/connection.py:803\u001b[39m, in \u001b[36mConnection._connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m803\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msocket.getaddrinfo returned an empty list\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/connection.py:787\u001b[39m, in \u001b[36mConnection._connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msocket_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# set the socket_timeout now that we're connected\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\u001b[39;00m\n\u001b[32m     10\u001b[39m DB_URI = \u001b[33m\"\u001b[39m\u001b[33mredis://localhost:6379\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mRedisStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_conn_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDB_URI\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mRedisSaver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_conn_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDB_URI\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langgraph/store/redis/__init__.py:118\u001b[39m, in \u001b[36mRedisStore.from_conn_string\u001b[39m\u001b[34m(cls, conn_string, index, ttl, store_prefix, vector_prefix)\u001b[39m\n\u001b[32m    116\u001b[39m client = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     client = \u001b[43mRedisConnectionFactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_redis_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     store = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    120\u001b[39m         client,\n\u001b[32m    121\u001b[39m         index=index,\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m         vector_prefix=vector_prefix,\n\u001b[32m    125\u001b[39m     )\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# Client info is set in __init__, but set it again here to ensure\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# it's available even if called before setup()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redisvl/redis/connection.py:378\u001b[39m, in \u001b[36mRedisConnectionFactory.get_redis_connection\u001b[39m\u001b[34m(redis_url, **kwargs)\u001b[39m\n\u001b[32m    376\u001b[39m _lib_name = make_lib_name(kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mlib_name\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient_setinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLIB-NAME\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lib_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ResponseError:\n\u001b[32m    380\u001b[39m     \u001b[38;5;66;03m# Fall back to a simple log echo\u001b[39;00m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(client, \u001b[33m\"\u001b[39m\u001b[33mecho\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/commands/core.py:716\u001b[39m, in \u001b[36mManagementCommands.client_setinfo\u001b[39m\u001b[34m(self, attr, value, **kwargs)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclient_setinfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m, value: \u001b[38;5;28mstr\u001b[39m, **kwargs) -> ResponseT:\n\u001b[32m    712\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[33;03m    Sets the current connection library name or version\u001b[39;00m\n\u001b[32m    714\u001b[39m \u001b[33;03m    For mor information see https://redis.io/commands/client-setinfo\u001b[39;00m\n\u001b[32m    715\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCLIENT SETINFO\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/client.py:621\u001b[39m, in \u001b[36mRedis.execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **options):\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/client.py:627\u001b[39m, in \u001b[36mRedis._execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    625\u001b[39m pool = \u001b[38;5;28mself\u001b[39m.connection_pool\n\u001b[32m    626\u001b[39m command_name = args[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m conn = \u001b[38;5;28mself\u001b[39m.connection \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._single_connection_client:\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m.single_connection_lock.acquire()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/utils.py:195\u001b[39m, in \u001b[36mdeprecated_args.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m provided_args:\n\u001b[32m    191\u001b[39m         warn_deprecated_arg_usage(\n\u001b[32m    192\u001b[39m             arg, func.\u001b[34m__name__\u001b[39m, reason, version, stacklevel=\u001b[32m3\u001b[39m\n\u001b[32m    193\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/connection.py:1533\u001b[39m, in \u001b[36mConnectionPool.get_connection\u001b[39m\u001b[34m(self, command_name, *keys, **options)\u001b[39m\n\u001b[32m   1529\u001b[39m     \u001b[38;5;28mself\u001b[39m._in_use_connections.add(connection)\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1532\u001b[39m     \u001b[38;5;66;03m# ensure this connection is connected to Redis\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1533\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1534\u001b[39m     \u001b[38;5;66;03m# connections that the pool provides should be ready to send\u001b[39;00m\n\u001b[32m   1535\u001b[39m     \u001b[38;5;66;03m# a command. if not, the connection was either returned to the\u001b[39;00m\n\u001b[32m   1536\u001b[39m     \u001b[38;5;66;03m# pool before all data has been read or the socket has been\u001b[39;00m\n\u001b[32m   1537\u001b[39m     \u001b[38;5;66;03m# closed. either way, reconnect and verify everything is good.\u001b[39;00m\n\u001b[32m   1538\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/connection.py:380\u001b[39m, in \u001b[36mAbstractConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    379\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mConnects to the Redis server if not already connected\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect_check_health\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_health\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/redis/connection.py:397\u001b[39m, in \u001b[36mAbstractConnection.connect_check_health\u001b[39m\u001b[34m(self, check_health, retry_socket_connect)\u001b[39m\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTimeout connecting to server\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m._error_message(e))\n\u001b[32m    399\u001b[39m \u001b[38;5;28mself\u001b[39m._sock = sock\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mConnectionError\u001b[39m: Error 111 connecting to localhost:6379. Connection refused."
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "from langgraph.store.redis import RedisStore  \n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "# model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n",
    "\n",
    "DB_URI = \"redis://localhost:6379\"\n",
    "\n",
    "with (\n",
    "    RedisStore.from_conn_string(DB_URI) as store,  \n",
    "    RedisSaver.from_conn_string(DB_URI) as checkpointer,\n",
    "):\n",
    "    store.setup()\n",
    "    checkpointer.setup()\n",
    "\n",
    "    def call_model(\n",
    "        state: MessagesState,\n",
    "        config: RunnableConfig,\n",
    "        *,\n",
    "        store: BaseStore,  \n",
    "    ):\n",
    "        user_id = config[\"configurable\"][\"user_id\"]\n",
    "        namespace = (\"memories\", user_id)\n",
    "        memories = store.search(namespace, query=str(state[\"messages\"][-1].content))  \n",
    "        info = \"\\n\".join([d.value[\"data\"] for d in memories])\n",
    "        system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n",
    "\n",
    "        # Store new memories if the user asks the model to remember\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if \"remember\" in last_message.content.lower():\n",
    "            memory = \"User name is Bob\"\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"data\": memory})  \n",
    "\n",
    "        response = model.invoke(\n",
    "            [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "        )\n",
    "        return {\"messages\": response}\n",
    "\n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "\n",
    "    graph = builder.compile(\n",
    "        checkpointer=checkpointer,\n",
    "        store=store,  \n",
    "    )\n",
    "\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"1\",  \n",
    "            \"user_id\": \"1\",  \n",
    "        }\n",
    "    }\n",
    "    for chunk in graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n",
    "        config,  \n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"2\",  \n",
    "            \"user_id\": \"1\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for chunk in graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n",
    "        config,  \n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "345d6676-1dd6-470f-afed-e1ca87d6d5d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T14:06:44.475044Z",
     "iopub.status.busy": "2025-11-04T14:06:44.474836Z",
     "iopub.status.idle": "2025-11-04T14:07:16.863952Z",
     "shell.execute_reply": "2025-11-04T14:07:16.863229Z",
     "shell.execute_reply.started": "2025-11-04T14:06:44.475031Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m embeddings = init_embeddings(\u001b[33m\"\u001b[39m\u001b[33mopenai:text-embedding-3-small\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m store = InMemoryStore(\n\u001b[32m      7\u001b[39m     index={\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33membed\u001b[39m\u001b[33m\"\u001b[39m: embeddings,\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdims\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1536\u001b[39m,\n\u001b[32m     10\u001b[39m     }\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_123\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemories\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mI love pizza\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m store.put((\u001b[33m\"\u001b[39m\u001b[33muser_123\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmemories\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33m2\u001b[39m\u001b[33m\"\u001b[39m, {\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mI am a plumber\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     16\u001b[39m items = store.search(\n\u001b[32m     17\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33muser_123\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmemories\u001b[39m\u001b[33m\"\u001b[39m), query=\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm hungry\u001b[39m\u001b[33m\"\u001b[39m, limit=\u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langgraph/store/base/__init__.py:917\u001b[39m, in \u001b[36mput\u001b[39m\u001b[34m(self, namespace, key, value, index, ttl)\u001b[39m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist_namespaces\u001b[39m(\n\u001b[32m    903\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    904\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    909\u001b[39m     offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m    910\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, ...]]:\n\u001b[32m    911\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"List and filter namespaces in the store.\u001b[39;00m\n\u001b[32m    912\u001b[39m \n\u001b[32m    913\u001b[39m \u001b[33;03m    Used to explore the organization of data,\u001b[39;00m\n\u001b[32m    914\u001b[39m \u001b[33;03m    find specific collections, or navigate the namespace hierarchy.\u001b[39;00m\n\u001b[32m    915\u001b[39m \n\u001b[32m    916\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m \u001b[33;03m        prefix: Filter namespaces that start with this path.\u001b[39;00m\n\u001b[32m    918\u001b[39m \u001b[33;03m        suffix: Filter namespaces that end with this path.\u001b[39;00m\n\u001b[32m    919\u001b[39m \u001b[33;03m        max_depth: Return namespaces up to this depth in the hierarchy.\u001b[39;00m\n\u001b[32m    920\u001b[39m \u001b[33;03m            Namespaces deeper than this level will be truncated.\u001b[39;00m\n\u001b[32m    921\u001b[39m \u001b[33;03m        limit: Maximum number of namespaces to return (default 100).\u001b[39;00m\n\u001b[32m    922\u001b[39m \u001b[33;03m        offset: Number of namespaces to skip for pagination (default 0).\u001b[39;00m\n\u001b[32m    923\u001b[39m \n\u001b[32m    924\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[33;03m        List[Tuple[str, ...]]: A list of namespace tuples that match the criteria.\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[33;03m        Each tuple represents a full namespace path up to `max_depth`.\u001b[39;00m\n\u001b[32m    927\u001b[39m \n\u001b[32m    928\u001b[39m \u001b[33;03m    ???+ example \"Examples\":\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[33;03m        Setting max_depth=3. Given the namespaces:\u001b[39;00m\n\u001b[32m    930\u001b[39m \u001b[33;03m        ```python\u001b[39;00m\n\u001b[32m    931\u001b[39m \u001b[33;03m        # Example if you have the following namespaces:\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[33;03m        # (\"a\", \"b\", \"c\")\u001b[39;00m\n\u001b[32m    933\u001b[39m \u001b[33;03m        # (\"a\", \"b\", \"d\", \"e\")\u001b[39;00m\n\u001b[32m    934\u001b[39m \u001b[33;03m        # (\"a\", \"b\", \"d\", \"i\")\u001b[39;00m\n\u001b[32m    935\u001b[39m \u001b[33;03m        # (\"a\", \"b\", \"f\")\u001b[39;00m\n\u001b[32m    936\u001b[39m \u001b[33;03m        # (\"a\", \"c\", \"f\")\u001b[39;00m\n\u001b[32m    937\u001b[39m \u001b[33;03m        store.list_namespaces(prefix=(\"a\", \"b\"), max_depth=3)\u001b[39;00m\n\u001b[32m    938\u001b[39m \u001b[33;03m        # [(\"a\", \"b\", \"c\"), (\"a\", \"b\", \"d\"), (\"a\", \"b\", \"f\")]\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[33;03m        ```\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    941\u001b[39m     match_conditions = []\n\u001b[32m    942\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prefix:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langgraph/store/memory/__init__.py:216\u001b[39m, in \u001b[36mInMemoryStore.batch\u001b[39m\u001b[34m(self, ops)\u001b[39m\n\u001b[32m    214\u001b[39m to_embed = \u001b[38;5;28mself\u001b[39m._extract_texts(put_ops)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_embed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.index_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embeddings:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mto_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m._insertinmem_store(to_embed, embeddings)\n\u001b[32m    218\u001b[39m \u001b[38;5;28mself\u001b[39m._apply_put_ops(put_ops)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:625\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    623\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    624\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:514\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    518\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/resources/embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_lock:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m         ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m         http2_negotiated = (\n\u001b[32m     82\u001b[39m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object.selected_alpn_protocol() == \u001b[33m\"\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    116\u001b[39m     kwargs = {\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhost\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.host.decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    118\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mport\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.port,\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msocket_options\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._socket_options,\n\u001b[32m    122\u001b[39m     }\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m         trace.return_value = stream\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_backends/sync.py:208\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     sock = \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n\u001b[32m    214\u001b[39m         sock.setsockopt(*option)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/socket.py:848\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[32m    847\u001b[39m     sock.bind(source_address)\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[32m    850\u001b[39m exceptions.clear()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Create store with semantic search enabled\n",
    "embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings,\n",
    "        \"dims\": 1536,\n",
    "    }\n",
    ")\n",
    "\n",
    "store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n",
    "store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n",
    "\n",
    "items = store.search(\n",
    "    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c37ba8f0-ed1a-4691-8f1a-5066fb91916d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T14:08:04.517692Z",
     "iopub.status.busy": "2025-11-04T14:08:04.517470Z",
     "iopub.status.idle": "2025-11-04T14:08:18.841178Z",
     "shell.execute_reply": "2025-11-04T14:08:18.840720Z",
     "shell.execute_reply.started": "2025-11-04T14:08:04.517678Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:08:04 httpx INFO   HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "22:08:07 httpx INFO   HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "22:08:12 httpx INFO   HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "22:08:16 httpx INFO   HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to your name in this conversation! You haven't told me yet. 😊\n",
      "\n",
      "If you'd like to tell me your name, I'd be happy to use it. Otherwise, I'll just refer to you in a friendly, general way.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages.utils import (\n",
    "    trim_messages,  \n",
    "    count_tokens_approximately  \n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "\n",
    "summarization_model = model.bind(max_tokens=128)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = trim_messages(  \n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=128,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff7975-aa93-4dff-a423-2d60b3ed828f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T14:08:55.216618Z",
     "iopub.status.busy": "2025-11-04T14:08:55.216358Z",
     "iopub.status.idle": "2025-11-04T14:08:58.898099Z",
     "shell.execute_reply": "2025-11-04T14:08:58.897672Z",
     "shell.execute_reply.started": "2025-11-04T14:08:55.216601Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('human', \"hi! I'm bob\")]\n",
      "22:08:55 httpx INFO   HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. 😊  \\nWhat brings you here today? Looking for help with something, or just saying hello?')]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. 😊  \\nWhat brings you here today? Looking for help with something, or just saying hello?'), ('human', \"what's my name?\")]\n",
      "22:08:57 httpx INFO   HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. 😊  \\nWhat brings you here today? Looking for help with something, or just saying hello?'), ('human', \"what's my name?\"), ('ai', \"You told me your name is Bob! 😊  \\nUnless you'd like me to call you something else?\")]\n",
      "[('human', \"what's my name?\"), ('ai', \"You told me your name is Bob! 😊  \\nUnless you'd like me to call you something else?\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import RemoveMessage  \n",
    "\n",
    "def delete_messages(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 2:\n",
    "        # remove the earliest two messages\n",
    "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_sequence([call_model, delete_messages])\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b894d04-0bf5-4522-9b0a-7fa481191291",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langmem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff539d-e527-4075-92ab-eb437d1a7023",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T14:16:06.529495Z",
     "iopub.status.busy": "2025-11-04T14:16:06.529253Z",
     "iopub.status.idle": "2025-11-04T14:16:23.968889Z",
     "shell.execute_reply": "2025-11-04T14:16:23.968438Z",
     "shell.execute_reply.started": "2025-11-04T14:16:06.529478Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:16:06 httpx INFO   HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "22:16:09 httpx INFO   HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "22:16:13 httpx INFO   HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "22:16:18 httpx INFO   HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "22:16:22 httpx INFO   HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. You mentioned it at the beginning of our conversation.\n",
      "\n",
      "Summary: Of course. Here is a summary of the conversation:\n",
      "\n",
      "The user, Bob, first requested a short poem about cats. The response was an eight-line poem highlighting feline traits like grace, mystery, and independence.\n",
      "\n",
      "Bob then asked for a similar poem about dogs. The response provided a companion eight-line poem that contrasted dogs' qualities—such as their loyalty, expressive joy, and playful nature—with the previously described nature of cats.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, TypedDict\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import AnyMessage\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langmem.short_term import SummarizationNode, RunningSummary  \n",
    "\n",
    "# model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n",
    "summarization_model = model.bind(max_tokens=128)\n",
    "\n",
    "class State(MessagesState):\n",
    "    context: dict[str, RunningSummary]  \n",
    "\n",
    "class LLMInputState(TypedDict):  \n",
    "    summarized_messages: list[AnyMessage]\n",
    "    context: dict[str, RunningSummary]\n",
    "\n",
    "summarization_node = SummarizationNode(  \n",
    "    token_counter=count_tokens_approximately,\n",
    "    model=summarization_model,\n",
    "    max_tokens=256,\n",
    "    max_tokens_before_summary=256,\n",
    "    max_summary_tokens=128,\n",
    ")\n",
    "\n",
    "def call_model(state: LLMInputState):  \n",
    "    response = model.invoke(state[\"summarized_messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(call_model)\n",
    "builder.add_node(\"summarize\", summarization_node)  \n",
    "builder.add_edge(START, \"summarize\")\n",
    "builder.add_edge(\"summarize\", \"call_model\")\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Invoke the graph\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "print(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9677c-dcc2-4164-aead-227631d27c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T14:16:27.158958Z",
     "iopub.status.busy": "2025-11-04T14:16:27.158750Z",
     "iopub.status.idle": "2025-11-04T14:16:27.162920Z",
     "shell.execute_reply": "2025-11-04T14:16:27.162469Z",
     "shell.execute_reply.started": "2025-11-04T14:16:27.158944Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38'), AIMessage(content=\"Of course, Bob! Here's a companion poem for our canine friends.\\n\\nA happy bark, a wagging tail,\\nA story that they'll never fail.\\nTo tell you with a slobbery kiss,\\nTheir simple, pure, and total bliss.\\n\\nA loyal friend, by your side they stay,\\nTo join you in your work and play.\\nThey love you more than words can tell,\\nAnd break the spell with a funny smell.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 160, 'total_tokens': 248, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 64}, 'prompt_cache_hit_tokens': 64, 'prompt_cache_miss_tokens': 96}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'aa5bbe6b-8524-4f28-b37a-55bde7149a24', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0', usage_metadata={'input_tokens': 160, 'output_tokens': 88, 'total_tokens': 248, 'input_token_details': {'cache_read': 64}, 'output_token_details': {}}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='4134e32e-9a58-41c1-9a7b-dca1e7d2ec56'), AIMessage(content='Your name is Bob. You mentioned it at the beginning of our conversation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 102, 'total_tokens': 117, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 102}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '236a946d-dcd7-4866-8153-a6abf19e2057', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--d4d0664f-e91e-4977-a7bb-2786ca7ed2a0-0', usage_metadata={'input_tokens': 102, 'output_tokens': 15, 'total_tokens': 117, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})], 'context': {'running_summary': RunningSummary(summary=\"Of course. Here is a summary of the conversation:\\n\\nThe user, Bob, first requested a short poem about cats. The response was an eight-line poem highlighting feline traits like grace, mystery, and independence.\\n\\nBob then asked for a similar poem about dogs. The response provided a companion eight-line poem that contrasted dogs' qualities—such as their loyalty, expressive joy, and playful nature—with the previously described nature of cats.\", summarized_message_ids={'98a0de9e-3384-4610-8acc-aa1500590c38', 'aeb3a5e7-8cfc-47a3-8403-d2589a249564', '3ae99b5c-74a1-465f-ab23-265b958ad7a4', 'lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', 'lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0', 'lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0'}, last_summarized_message_id='lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0')}, 'summarized_messages': [SystemMessage(content=\"Summary of the conversation so far: Of course. Here is a summary of the conversation:\\n\\nThe user, Bob, first requested a short poem about cats. The response was an eight-line poem highlighting feline traits like grace, mystery, and independence.\\n\\nBob then asked for a similar poem about dogs. The response provided a companion eight-line poem that contrasted dogs' qualities—such as their loyalty, expressive joy, and playful nature—with the previously described nature of cats.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='4134e32e-9a58-41c1-9a7b-dca1e7d2ec56')]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-8094-66fa-800e-70b0e16b32eb'}}, metadata={'source': 'loop', 'step': 14, 'parents': {}}, created_at='2025-11-04T14:16:23.966263+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-70b2-65a9-800d-d16aed7236ba'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"1\",  \n",
    "        # optionally provide an ID for a specific checkpoint,\n",
    "        # otherwise the latest checkpoint is shown\n",
    "        # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  #\n",
    "\n",
    "    }\n",
    "}\n",
    "graph.get_state(config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac948dc-b8cc-4fe0-af7e-5e130cd0c5d2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T14:16:32.389886Z",
     "iopub.status.busy": "2025-11-04T14:16:32.389677Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38'), AIMessage(content=\"Of course, Bob! Here's a companion poem for our canine friends.\\n\\nA happy bark, a wagging tail,\\nA story that they'll never fail.\\nTo tell you with a slobbery kiss,\\nTheir simple, pure, and total bliss.\\n\\nA loyal friend, by your side they stay,\\nTo join you in your work and play.\\nThey love you more than words can tell,\\nAnd break the spell with a funny smell.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 160, 'total_tokens': 248, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 64}, 'prompt_cache_hit_tokens': 64, 'prompt_cache_miss_tokens': 96}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'aa5bbe6b-8524-4f28-b37a-55bde7149a24', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0', usage_metadata={'input_tokens': 160, 'output_tokens': 88, 'total_tokens': 248, 'input_token_details': {'cache_read': 64}, 'output_token_details': {}}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='4134e32e-9a58-41c1-9a7b-dca1e7d2ec56'), AIMessage(content='Your name is Bob. You mentioned it at the beginning of our conversation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 102, 'total_tokens': 117, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 102}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '236a946d-dcd7-4866-8153-a6abf19e2057', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--d4d0664f-e91e-4977-a7bb-2786ca7ed2a0-0', usage_metadata={'input_tokens': 102, 'output_tokens': 15, 'total_tokens': 117, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})], 'context': {'running_summary': RunningSummary(summary=\"Of course. Here is a summary of the conversation:\\n\\nThe user, Bob, first requested a short poem about cats. The response was an eight-line poem highlighting feline traits like grace, mystery, and independence.\\n\\nBob then asked for a similar poem about dogs. The response provided a companion eight-line poem that contrasted dogs' qualities—such as their loyalty, expressive joy, and playful nature—with the previously described nature of cats.\", summarized_message_ids={'98a0de9e-3384-4610-8acc-aa1500590c38', 'aeb3a5e7-8cfc-47a3-8403-d2589a249564', '3ae99b5c-74a1-465f-ab23-265b958ad7a4', 'lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', 'lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0', 'lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0'}, last_summarized_message_id='lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0')}, 'summarized_messages': [SystemMessage(content=\"Summary of the conversation so far: Of course. Here is a summary of the conversation:\\n\\nThe user, Bob, first requested a short poem about cats. The response was an eight-line poem highlighting feline traits like grace, mystery, and independence.\\n\\nBob then asked for a similar poem about dogs. The response provided a companion eight-line poem that contrasted dogs' qualities—such as their loyalty, expressive joy, and playful nature—with the previously described nature of cats.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='4134e32e-9a58-41c1-9a7b-dca1e7d2ec56')]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-8094-66fa-800e-70b0e16b32eb'}}, metadata={'source': 'loop', 'step': 14, 'parents': {}}, created_at='2025-11-04T14:16:23.966263+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-70b2-65a9-800d-d16aed7236ba'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38'), AIMessage(content=\"Of course, Bob! Here's a companion poem for our canine friends.\\n\\nA happy bark, a wagging tail,\\nA story that they'll never fail.\\nTo tell you with a slobbery kiss,\\nTheir simple, pure, and total bliss.\\n\\nA loyal friend, by your side they stay,\\nTo join you in your work and play.\\nThey love you more than words can tell,\\nAnd break the spell with a funny smell.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 160, 'total_tokens': 248, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 64}, 'prompt_cache_hit_tokens': 64, 'prompt_cache_miss_tokens': 96}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'aa5bbe6b-8524-4f28-b37a-55bde7149a24', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0', usage_metadata={'input_tokens': 160, 'output_tokens': 88, 'total_tokens': 248, 'input_token_details': {'cache_read': 64}, 'output_token_details': {}}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='4134e32e-9a58-41c1-9a7b-dca1e7d2ec56')], 'context': {'running_summary': RunningSummary(summary=\"Of course. Here is a summary of the conversation:\\n\\nThe user, Bob, first requested a short poem about cats. The response was an eight-line poem highlighting feline traits like grace, mystery, and independence.\\n\\nBob then asked for a similar poem about dogs. The response provided a companion eight-line poem that contrasted dogs' qualities—such as their loyalty, expressive joy, and playful nature—with the previously described nature of cats.\", summarized_message_ids={'98a0de9e-3384-4610-8acc-aa1500590c38', 'aeb3a5e7-8cfc-47a3-8403-d2589a249564', '3ae99b5c-74a1-465f-ab23-265b958ad7a4', 'lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', 'lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0', 'lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0'}, last_summarized_message_id='lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0')}, 'summarized_messages': [SystemMessage(content=\"Summary of the conversation so far: Of course. Here is a summary of the conversation:\\n\\nThe user, Bob, first requested a short poem about cats. The response was an eight-line poem highlighting feline traits like grace, mystery, and independence.\\n\\nBob then asked for a similar poem about dogs. The response provided a companion eight-line poem that contrasted dogs' qualities—such as their loyalty, expressive joy, and playful nature—with the previously described nature of cats.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='4134e32e-9a58-41c1-9a7b-dca1e7d2ec56')]}, next=('call_model',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-70b2-65a9-800d-d16aed7236ba'}}, metadata={'source': 'loop', 'step': 13, 'parents': {}}, created_at='2025-11-04T14:16:22.300797+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-47b8-6497-800c-22944882f361'}}, tasks=(PregelTask(id='25d4ec39-6443-b99a-91fc-9c05b0c50366', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': [AIMessage(content='Your name is Bob. You mentioned it at the beginning of our conversation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 102, 'total_tokens': 117, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 102}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '236a946d-dcd7-4866-8153-a6abf19e2057', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--d4d0664f-e91e-4977-a7bb-2786ca7ed2a0-0', usage_metadata={'input_tokens': 102, 'output_tokens': 15, 'total_tokens': 117, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38'), AIMessage(content=\"Of course, Bob! Here's a companion poem for our canine friends.\\n\\nA happy bark, a wagging tail,\\nA story that they'll never fail.\\nTo tell you with a slobbery kiss,\\nTheir simple, pure, and total bliss.\\n\\nA loyal friend, by your side they stay,\\nTo join you in your work and play.\\nThey love you more than words can tell,\\nAnd break the spell with a funny smell.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 160, 'total_tokens': 248, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 64}, 'prompt_cache_hit_tokens': 64, 'prompt_cache_miss_tokens': 96}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'aa5bbe6b-8524-4f28-b37a-55bde7149a24', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0', usage_metadata={'input_tokens': 160, 'output_tokens': 88, 'total_tokens': 248, 'input_token_details': {'cache_read': 64}, 'output_token_details': {}}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='4134e32e-9a58-41c1-9a7b-dca1e7d2ec56')], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38')]}, next=('summarize',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-47b8-6497-800c-22944882f361'}}, metadata={'source': 'loop', 'step': 12, 'parents': {}}, created_at='2025-11-04T14:16:18.004075+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-47b7-626f-800b-3af3f2de0f27'}}, tasks=(PregelTask(id='537d328e-6b85-47ff-7b18-9c9f44528d5d', name='summarize', path=('__pregel_pull', 'summarize'), error=None, interrupts=(), state=None, result={'summarized_messages': [SystemMessage(content=\"Summary of the conversation so far: Of course. Here is a summary of the conversation:\\n\\nThe user, Bob, first requested a short poem about cats. The response was an eight-line poem highlighting feline traits like grace, mystery, and independence.\\n\\nBob then asked for a similar poem about dogs. The response provided a companion eight-line poem that contrasted dogs' qualities—such as their loyalty, expressive joy, and playful nature—with the previously described nature of cats.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='4134e32e-9a58-41c1-9a7b-dca1e7d2ec56')], 'context': {'running_summary': RunningSummary(summary=\"Of course. Here is a summary of the conversation:\\n\\nThe user, Bob, first requested a short poem about cats. The response was an eight-line poem highlighting feline traits like grace, mystery, and independence.\\n\\nBob then asked for a similar poem about dogs. The response provided a companion eight-line poem that contrasted dogs' qualities—such as their loyalty, expressive joy, and playful nature—with the previously described nature of cats.\", summarized_message_ids={'98a0de9e-3384-4610-8acc-aa1500590c38', 'aeb3a5e7-8cfc-47a3-8403-d2589a249564', '3ae99b5c-74a1-465f-ab23-265b958ad7a4', 'lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', 'lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0', 'lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0'}, last_summarized_message_id='lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0')}}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38'), AIMessage(content=\"Of course, Bob! Here's a companion poem for our canine friends.\\n\\nA happy bark, a wagging tail,\\nA story that they'll never fail.\\nTo tell you with a slobbery kiss,\\nTheir simple, pure, and total bliss.\\n\\nA loyal friend, by your side they stay,\\nTo join you in your work and play.\\nThey love you more than words can tell,\\nAnd break the spell with a funny smell.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 160, 'total_tokens': 248, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 64}, 'prompt_cache_hit_tokens': 64, 'prompt_cache_miss_tokens': 96}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'aa5bbe6b-8524-4f28-b37a-55bde7149a24', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0', usage_metadata={'input_tokens': 160, 'output_tokens': 88, 'total_tokens': 248, 'input_token_details': {'cache_read': 64}, 'output_token_details': {}})], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38')]}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-47b7-626f-800b-3af3f2de0f27'}}, metadata={'source': 'input', 'step': 11, 'parents': {}}, created_at='2025-11-04T14:16:18.003607+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-47b4-6380-800a-d3fb3741673d'}}, tasks=(PregelTask(id='c56804c7-dc2e-c7fb-0f9c-eac8a48bbe66', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': \"what's my name?\"}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38'), AIMessage(content=\"Of course, Bob! Here's a companion poem for our canine friends.\\n\\nA happy bark, a wagging tail,\\nA story that they'll never fail.\\nTo tell you with a slobbery kiss,\\nTheir simple, pure, and total bliss.\\n\\nA loyal friend, by your side they stay,\\nTo join you in your work and play.\\nThey love you more than words can tell,\\nAnd break the spell with a funny smell.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 160, 'total_tokens': 248, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 64}, 'prompt_cache_hit_tokens': 64, 'prompt_cache_miss_tokens': 96}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'aa5bbe6b-8524-4f28-b37a-55bde7149a24', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0', usage_metadata={'input_tokens': 160, 'output_tokens': 88, 'total_tokens': 248, 'input_token_details': {'cache_read': 64}, 'output_token_details': {}})], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38')]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-47b4-6380-800a-d3fb3741673d'}}, metadata={'source': 'loop', 'step': 10, 'parents': {}}, created_at='2025-11-04T14:16:18.002399+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-1d21-6ae3-8009-a62719b2938e'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38')], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38')]}, next=('call_model',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-1d21-6ae3-8009-a62719b2938e'}}, metadata={'source': 'loop', 'step': 9, 'parents': {}}, created_at='2025-11-04T14:16:13.538368+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-1d20-658d-8008-6a6964759460'}}, tasks=(PregelTask(id='5c90e726-7b4a-f4b0-647a-7928e86cd2d3', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': [AIMessage(content=\"Of course, Bob! Here's a companion poem for our canine friends.\\n\\nA happy bark, a wagging tail,\\nA story that they'll never fail.\\nTo tell you with a slobbery kiss,\\nTheir simple, pure, and total bliss.\\n\\nA loyal friend, by your side they stay,\\nTo join you in your work and play.\\nThey love you more than words can tell,\\nAnd break the spell with a funny smell.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 160, 'total_tokens': 248, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 64}, 'prompt_cache_hit_tokens': 64, 'prompt_cache_miss_tokens': 96}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'aa5bbe6b-8524-4f28-b37a-55bde7149a24', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--2ba3db98-87d0-4b43-976d-0f4705175ce6-0', usage_metadata={'input_tokens': 160, 'output_tokens': 88, 'total_tokens': 248, 'input_token_details': {'cache_read': 64}, 'output_token_details': {}})]}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38')], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4')]}, next=('summarize',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-1d20-658d-8008-6a6964759460'}}, metadata={'source': 'loop', 'step': 8, 'parents': {}}, created_at='2025-11-04T14:16:13.537821+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-1d1f-61d4-8007-884bdd2e1bdd'}}, tasks=(PregelTask(id='b16dbabc-2d7e-e83a-4a5e-f09320a49215', name='summarize', path=('__pregel_pull', 'summarize'), error=None, interrupts=(), state=None, result={'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='98a0de9e-3384-4610-8acc-aa1500590c38')]}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4')]}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-1d1f-61d4-8007-884bdd2e1bdd'}}, metadata={'source': 'input', 'step': 7, 'parents': {}}, created_at='2025-11-04T14:16:13.537314+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-1d1c-664e-8006-256cbeed8bd1'}}, tasks=(PregelTask(id='74710b8b-d0bb-b3de-84c6-480287766ec2', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': 'now do the same but for dogs'}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4'), AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4')]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988d-1d1c-664e-8006-256cbeed8bd1'}}, metadata={'source': 'loop', 'step': 6, 'parents': {}}, created_at='2025-11-04T14:16:13.536189+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-f352-660a-8005-f34094ef1d5e'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4')], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4')]}, next=('call_model',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-f352-660a-8005-f34094ef1d5e'}}, metadata={'source': 'loop', 'step': 5, 'parents': {}}, created_at='2025-11-04T14:16:09.154295+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-f351-6075-8004-ca8f6a8d0b6f'}}, tasks=(PregelTask(id='f4427e95-fdbf-3491-2493-220ba7b14b85', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': [AIMessage(content=\"Of course! Here's a short poem for you, Bob.\\n\\nA soft paw, a graceful leap,\\nWhile the rest of the world is asleep.\\nThey rule the house with silent pride,\\nWith nowhere left, and no need to hide.\\n\\nA purr that rumbles, deep and low,\\nA mystery that they'll never show.\\nThey judge us from a sunbeam's spot,\\nAnd love us dearly... or maybe not.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 63, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a55b1707-6f6a-4508-aa74-8ebee5e8062b', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7fc5b12b-3689-4d57-a1cd-12668b7388e6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 86, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4')], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564')]}, next=('summarize',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-f351-6075-8004-ca8f6a8d0b6f'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-11-04T14:16:09.153739+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-f34f-6a70-8003-53f5fdd9668d'}}, tasks=(PregelTask(id='b2f1e227-e393-2b3e-30ed-057d27218321', name='summarize', path=('__pregel_pull', 'summarize'), error=None, interrupts=(), state=None, result={'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='3ae99b5c-74a1-465f-ab23-265b958ad7a4')]}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564')]}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-f34f-6a70-8003-53f5fdd9668d'}}, metadata={'source': 'input', 'step': 3, 'parents': {}}, created_at='2025-11-04T14:16:09.153175+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-f34d-6112-8002-3ae2d16ca881'}}, tasks=(PregelTask(id='b2603e36-56a6-31f8-13ad-320393268dcf', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': 'write a short poem about cats'}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564'), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564')]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-f34d-6112-8002-3ae2d16ca881'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-11-04T14:16:09.152112+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-da5d-6c97-8001-59062477e783'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564')], 'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564')]}, next=('call_model',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-da5d-6c97-8001-59062477e783'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-11-04T14:16:06.537525+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-da5c-627e-8000-5946b1434141'}}, tasks=(PregelTask(id='fa1e81fc-f40d-52c4-74bf-5fedc187f6b4', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': [AIMessage(content=\"Hi Bob! It's nice to meet you. 😊\\n\\nIs there anything I can help you with today? Whether it's a question, a task, or just a chat, I'm here for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 10, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'fb7cb30f-9e1c-45bc-9a95-4e2643843eb5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9a8a2195-b87e-4497-a348-00b4959ba7ee-0', usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564')]}, next=('summarize',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-da5c-627e-8000-5946b1434141'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-11-04T14:16:06.536860+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-da5a-673e-bfff-74af5feacbc3'}}, tasks=(PregelTask(id='34ee4da4-a4a2-d07d-a7e0-b0da9a650a7a', name='summarize', path=('__pregel_pull', 'summarize'), error=None, interrupts=(), state=None, result={'summarized_messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='aeb3a5e7-8cfc-47a3-8403-d2589a249564')]}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': []}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b988c-da5a-673e-bfff-74af5feacbc3'}}, metadata={'source': 'input', 'step': -1, 'parents': {}}, created_at='2025-11-04T14:16:06.536165+00:00', parent_config=None, tasks=(PregelTask(id='ba6b6229-e5b9-f21d-3fda-e428c25daf2a', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': 'hi, my name is bob'}),), interrupts=())]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"1\"\n",
    "    }\n",
    "}\n",
    "list(graph.get_state_history(config))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6926c-663b-4322-9612-61a5f805dfb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T14:15:33.311943Z",
     "iopub.status.busy": "2025-11-04T14:15:33.311733Z",
     "iopub.status.idle": "2025-11-04T14:15:33.314276Z",
     "shell.execute_reply": "2025-11-04T14:15:33.313901Z",
     "shell.execute_reply.started": "2025-11-04T14:15:33.311929Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "thread_id = \"1\"\n",
    "checkpointer.delete_thread(thread_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
