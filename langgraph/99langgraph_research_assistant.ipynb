{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82783441-7946-4d43-9a55-c42f0b6b3ce7",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !uv add langgraph\n",
    "\n",
    "!pip install langchain langgraph\n",
    "!pip install -U langchain langgraph\n",
    "!pip install langchain-openai openai\n",
    "!pip install langchain-deepseek\n",
    "!pip install ddgs\n",
    "!pip install langchain-community wikipedia\n",
    "# !pip install -U langchain-community wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e75657-ee59-4434-a138-0fcb8a692deb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-11T12:33:25.287951Z",
     "iopub.status.busy": "2025-11-11T12:33:25.287726Z",
     "iopub.status.idle": "2025-11-11T12:33:28.210527Z",
     "shell.execute_reply": "2025-11-11T12:33:28.210050Z",
     "shell.execute_reply.started": "2025-11-11T12:33:25.287932Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "LangGraph 1.0 å¤æ‚ç¤ºä¾‹ï¼šæ™ºèƒ½ç ”ç©¶åŠ©æ‰‹\n",
    "åŠŸèƒ½ï¼šå¤šæ­¥éª¤ç ”ç©¶ã€åˆ†æã€éªŒè¯å’ŒæŠ¥å‘Šç”Ÿæˆ\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional, TypedDict, Annotated\n",
    "from datetime import datetime\n",
    "import operator\n",
    "\n",
    "# LangGraph 1.0 å¯¼å…¥\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# LangChain å¯¼å…¥\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "try:\n",
    "    # å°è¯•ä½¿ç”¨æ–°ç‰ˆæœ¬çš„å¯¼å…¥\n",
    "    from langchain_community.tools import WikipediaQueryRun\n",
    "    from langchain_community.utilities import WikipediaAPIWrapper\n",
    "    WIKIPEDIA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    # å¦‚æœæ–°ç‰ˆæœ¬ä¸å¯ç”¨ï¼Œä½¿ç”¨æ—§ç‰ˆæœ¬æˆ–æ›¿ä»£æ–¹æ¡ˆ\n",
    "    WIKIPEDIA_AVAILABLE = False\n",
    "    print(\"âš ï¸  Wikipediaå·¥å…·ä¸å¯ç”¨ï¼Œå°†ä»…ä½¿ç”¨DuckDuckGoæœç´¢\")\n",
    "\n",
    "# # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„å®é™…APIå¯†é’¥ï¼‰\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db97739f-c549-4110-9d79-87bd22288de1",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-11-11T12:33:30.934181Z",
     "iopub.status.busy": "2025-11-11T12:33:30.933877Z",
     "iopub.status.idle": "2025-11-11T12:33:31.212643Z",
     "shell.execute_reply": "2025-11-11T12:33:31.212170Z",
     "shell.execute_reply.started": "2025-11-11T12:33:30.934163Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# è®¾ç½® DeepSeek çš„ API å¯†é’¥ï¼ˆLangChain-OpenAI ä»ç„¶ä¼šæŸ¥æ‰¾ OPENAI_API_KEYï¼‰\n",
    "os.environ['DEEPSEEK_API_KEY'] = 'sk-943df854319e423ca178e68e4668ca5a'\n",
    "\n",
    "# æ‚¨å¯ä»¥å°è¯•å°† DEEPSEEK_API_KEY çš„å€¼èµ‹ç»™ OPENAI_API_KEY ç¯å¢ƒå˜é‡\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('DEEPSEEK_API_KEY') # ç¡®ä¿è¿™ä¸ªå€¼æ˜¯ DeepSeek çš„ key\n",
    "\n",
    "# å…³é”®ï¼šæŒ‡å®š DeepSeek çš„ API åŸºç¡€ URL\n",
    "DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\", # ä½¿ç”¨ DeepSeek çš„æ¨¡å‹åç§°\n",
    "    openai_api_base=DEEPSEEK_BASE_URL, # æŒ‡å®š DeepSeek çš„ URL\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "252c5732-0fd4-4d8e-82c4-7df14549f5f0",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-11-09T05:21:51.031980Z",
     "iopub.status.busy": "2025-11-09T05:21:51.031672Z",
     "iopub.status.idle": "2025-11-09T05:21:52.219104Z",
     "shell.execute_reply": "2025-11-09T05:21:52.218656Z",
     "shell.execute_reply.started": "2025-11-09T05:21:51.031963Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ å¤±è´¥ï¼š ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n"
     ]
    }
   ],
   "source": [
    "from ddgs import DDGS\n",
    "\n",
    "# # åˆå§‹åŒ–æœç´¢å·¥å…·\n",
    "# search_tool = DDGS()\n",
    "\n",
    "# # 1. æµ‹è¯•åº•å±‚åº“\n",
    "# print(search_tool.text(\"alibaba\", max_results=1))\n",
    "# results =search_tool.text(\"LangGraph\", max_results=1)\n",
    "# if results:\n",
    "#     print(\"Title:\", results[0][\"title\"])\n",
    "#     print(\"URL:\", results[0][\"href\"])\n",
    "#     print(\"Description:\", results[0][\"body\"][:100] + \"...\")\n",
    "# else:\n",
    "#     print(\"No results found.\")\n",
    "    \n",
    "    \n",
    "# # å¯é€‰ï¼šé…ç½®å‚æ•°\n",
    "# api_wrapper = WikipediaAPIWrapper(\n",
    "#     top_k_results=1,          # åªè¿”å›1ä¸ªç»“æœ\n",
    "#     doc_content_chars_max=500, # æ¯ç¯‡æœ€å¤š500å­—ç¬¦\n",
    "#     lang=\"zh\"                 # ä¸­æ–‡ç»´åŸºï¼ˆå¯é€‰ï¼‰\n",
    "# )   \n",
    "\n",
    "# # âœ… æ­£ç¡®å¯¼å…¥\n",
    "# from langchain_community.tools import WikipediaQueryRun\n",
    "# from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# wikipedia_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "# try:\n",
    "#     result = wikipedia_tool.invoke(\"Albert Einstein\")\n",
    "#     print(\"âœ… æˆåŠŸï¼ç»“æœé¢„è§ˆï¼š\")\n",
    "#     print(result[:300] + \"...\")\n",
    "# except Exception as e:\n",
    "#     print(\"âŒ å¤±è´¥ï¼š\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f502df6-b535-4a43-8166-f2f2569c98de",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-11T12:33:55.799930Z",
     "iopub.status.busy": "2025-11-11T12:33:55.799690Z",
     "iopub.status.idle": "2025-11-11T12:33:55.807389Z",
     "shell.execute_reply": "2025-11-11T12:33:55.806954Z",
     "shell.execute_reply.started": "2025-11-11T12:33:55.799912Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):    \n",
    "    # ç”¨æˆ·è¾“å…¥å­—æ®µ\n",
    "    topic: str\n",
    "    research_depth: str  # e.g., \"basic\", \"standard\", \"comprehensive\"\n",
    "    \n",
    "    # æœç´¢ä¸ç ”ç©¶ä¸­é—´æ•°æ®\n",
    "    search_queries: List[str]\n",
    "    search_results: List[Dict[str, Any]]  # å»ºè®®ç»“æ„ï¼š{\"title\": str, \"snippet\": str, \"url\": str, \"source_type\": str}\n",
    "    research_notes: List[str]\n",
    "    key_findings: List[str]\n",
    "    analysis_summary: str\n",
    "    research_gaps: List[str]\n",
    "    improvement_suggestions: List[str]\n",
    "    \n",
    "    # è¯„ä¼°ä¸è´¨é‡å­—æ®µ\n",
    "    credibility_scores: Optional[Dict[str, float]]  # e.g., {\"overall\": 0.85, \"source_diversity\": 0.8, ...}\n",
    "    verification_status: bool\n",
    "    quality_score: float\n",
    "    \n",
    "    # æŠ¥å‘Šè¾“å‡ºå­—æ®µ\n",
    "    final_report: str\n",
    "    executive_summary: str\n",
    "    \n",
    "    # çŠ¶æ€æ§åˆ¶å­—æ®µ\n",
    "    current_step: str\n",
    "    retry_count: int\n",
    "    max_retries: int\n",
    "    completed_steps: List[str]\n",
    "    \n",
    "    # æ¶ˆæ¯å†å²ï¼ˆç”¨äº LLM äº¤äº’ï¼‰\n",
    "    messages: List[Dict[str, Any]]  # æˆ–æ›´ä¸¥æ ¼ï¼šList[BaseMessage]ï¼Œä½† TypedDict é€šå¸¸ç”¨ dict\n",
    "    \n",
    "    \n",
    "    \n",
    "# ============================================================================\n",
    "# 3. å®ç°èŠ‚ç‚¹å‡½æ•° (Node Functions)\n",
    "# ============================================================================\n",
    "\n",
    "def initialize_research(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    èŠ‚ç‚¹1ï¼šåˆå§‹åŒ–ç ”ç©¶\n",
    "    åŠŸèƒ½ï¼šè§£æç ”ç©¶ä¸»é¢˜ï¼Œç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼Œè®¾ç½®ç ”ç©¶å‚æ•°\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ æ­¥éª¤1ï¼šåˆå§‹åŒ–ç ”ç©¶\")\n",
    "\n",
    "    # æ›´æ–°å½“å‰æ­¥éª¤\n",
    "    state[\"current_step\"] = \"initialization\"\n",
    "    state[\"completed_steps\"] = [\"initialization\"]\n",
    "\n",
    "    # ç”Ÿæˆæœç´¢æŸ¥è¯¢\n",
    "    topic = state[\"topic\"]\n",
    "    depth = state[\"research_depth\"]\n",
    "\n",
    "    # æ ¹æ®ç ”ç©¶æ·±åº¦ç”Ÿæˆä¸åŒæ•°é‡çš„æŸ¥è¯¢\n",
    "    if depth == \"basic\":\n",
    "        query_count = 3\n",
    "    elif depth == \"standard\":\n",
    "        query_count = 5\n",
    "    else:  # comprehensive\n",
    "        query_count = 8\n",
    "\n",
    "    # ä½¿ç”¨LLMç”Ÿæˆå¤šæ ·åŒ–çš„æœç´¢æŸ¥è¯¢\n",
    "    query_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    ä¸ºç ”ç©¶ä¸»é¢˜ï¼š\"{topic}\"\n",
    "    ç”Ÿæˆ {count} ä¸ªä¸åŒçš„æœç´¢æŸ¥è¯¢ï¼Œæ¶µç›–ä»¥ä¸‹è§’åº¦ï¼š\n",
    "    1. åŸºç¡€å®šä¹‰å’Œæ¦‚å¿µ\n",
    "    2. æœ€æ–°å‘å±•å’Œè¶‹åŠ¿\n",
    "    3. å…³é”®äººç‰©å’Œæœºæ„\n",
    "    4. äº‰è®®å’ŒæŒ‘æˆ˜\n",
    "    5. æœªæ¥å±•æœ›\n",
    "    ç ”ç©¶æ·±åº¦ï¼š{depth}\n",
    "\n",
    "    åªè¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªï¼š\n",
    "    \"\"\")\n",
    "\n",
    "    query_chain = query_prompt | llm\n",
    "    result = query_chain.invoke({\n",
    "        \"topic\": topic,\n",
    "        \"count\": query_count,\n",
    "        \"depth\": depth\n",
    "    })\n",
    "\n",
    "    # è§£ææŸ¥è¯¢\n",
    "    queries = [q.strip() for q in result.content.split('\\n') if q.strip()]\n",
    "    state[\"search_queries\"] = queries[:query_count]  # ç¡®ä¿ä¸è¶…è¿‡é¢„æœŸæ•°é‡\n",
    "\n",
    "    # åˆå§‹åŒ–å…¶ä»–å­—æ®µ\n",
    "    state[\"search_results\"] = []\n",
    "    state[\"research_notes\"] = []\n",
    "    state[\"key_findings\"] = []\n",
    "    state[\"research_gaps\"] = []\n",
    "    state[\"improvement_suggestions\"] = []\n",
    "    state[\"retry_count\"] = 0\n",
    "    state[\"max_retries\"] = 2\n",
    "\n",
    "    print(f\"âœ… ç”Ÿæˆäº† {len(state['search_queries'])} ä¸ªæœç´¢æŸ¥è¯¢\")\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "267878f6-2939-4444-bd2b-6f3dc22f8c95",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-11T12:33:59.924729Z",
     "iopub.status.busy": "2025-11-11T12:33:59.924486Z",
     "iopub.status.idle": "2025-11-11T12:33:59.945907Z",
     "shell.execute_reply": "2025-11-11T12:33:59.945430Z",
     "shell.execute_reply.started": "2025-11-11T12:33:59.924712Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conduct_search(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    èŠ‚ç‚¹2ï¼šæ‰§è¡Œæœç´¢\n",
    "    åŠŸèƒ½ï¼šä½¿ç”¨ç”Ÿæˆçš„æŸ¥è¯¢æ‰§è¡Œæœç´¢ï¼Œæ”¶é›†ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    print(\" æ­¥éª¤2: Conducting search\")\n",
    "\n",
    "    state[\"current_step\"] = \"searching\"\n",
    "    state[\"completed_steps\"].append(\"searching\")\n",
    "\n",
    "    search_results = []\n",
    "\n",
    "    for i, query in enumerate(state[\"search_queries\"]):\n",
    "        print(f\"  æ‰§è¡ŒæŸ¥è¯¢ {i+1}/{len(state['search_queries'])}: {query}\")\n",
    "\n",
    "        try:\n",
    "            # ä½¿ç”¨DuckDuckGoæœç´¢ï¼ˆæ–°ç‰ˆæœ¬APIï¼‰\n",
    "            ddg_result = \"\"\n",
    "            try:\n",
    "                # å°è¯•ä½¿ç”¨æ–°ç‰ˆæœ¬çš„ ddgs API\n",
    "                from ddgs import DDGS\n",
    "                ddgs = DDGS()\n",
    "                results = list(ddgs.text(query, max_results=5))\n",
    "                if results:\n",
    "                    ddg_result = \"\\n\".join([f\"- {result['title']}: {result['body']}\" for result in results])\n",
    "            except Exception:\n",
    "                # å¦‚æœæ–°APIå¤±è´¥ï¼Œå°è¯•æ—§ç‰ˆæœ¬çš„ langchain å·¥å…·\n",
    "                try:\n",
    "                    ddg_result = search_tool.run(query)\n",
    "                except Exception as e:\n",
    "                    raise Exception(f\"DuckDuckGo search failed: {str(e)}\")\n",
    "\n",
    "            # ä½¿ç”¨Wikipediaæœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "            wiki_result = \"\"\n",
    "            if wikipedia_tool:\n",
    "                try:\n",
    "                    wiki_result = wikipedia_tool.invoke(query)\n",
    "                    # é™åˆ¶ç»“æœé•¿åº¦\n",
    "                    if len(wiki_result) > 1000:\n",
    "                        wiki_result = wiki_result[:1000] + \"...\"\n",
    "                except Exception as e:\n",
    "                    print(f\"    âš ï¸  Wikipediaæœç´¢å¤±è´¥: {str(e)}\")\n",
    "                    wiki_result = \"\"\n",
    "\n",
    "            # å­˜å‚¨ç»“æœ\n",
    "            result_entry = {\n",
    "                \"query\": query,\n",
    "                \"ddg_result\": ddg_result,\n",
    "                \"wiki_result\": wiki_result,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"source_type\": \"search\"\n",
    "            }\n",
    "            search_results.append(result_entry)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    [FAIL] Search failed: {str(e)}\")\n",
    "            # è®°å½•å¤±è´¥çš„æœç´¢\n",
    "            search_results.append({\n",
    "                \"query\": query,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"source_type\": \"error\"\n",
    "            })\n",
    "\n",
    "    state[\"search_results\"] = search_results\n",
    "    print(f\"[OK] Completed {len(search_results)} search queries\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def analyze_findings(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    èŠ‚ç‚¹3ï¼šåˆ†æå‘ç°\n",
    "    åŠŸèƒ½ï¼šåˆ†ææœç´¢ç»“æœï¼Œæå–å…³é”®ä¿¡æ¯ï¼Œè¯„ä¼°å¯ä¿¡åº¦\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š æ­¥éª¤3ï¼šåˆ†æå‘ç°\")\n",
    "\n",
    "    state[\"current_step\"] = \"analysis\"\n",
    "    state[\"completed_steps\"].append(\"analysis\")\n",
    "\n",
    "    # å‡†å¤‡åˆ†ææç¤º\n",
    "    analysis_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    ä½œä¸ºä¸“ä¸šç ”ç©¶åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹æœç´¢ç»“æœï¼š\n",
    "\n",
    "    ç ”ç©¶ä¸»é¢˜ï¼š{topic}\n",
    "    ç ”ç©¶æ·±åº¦ï¼š{depth}\n",
    "\n",
    "    æœç´¢ç»“æœï¼š\n",
    "    {search_data}\n",
    "\n",
    "    è¯·æä¾›ï¼š\n",
    "    1. å…³é”®å‘ç°ï¼ˆ3-5ä¸ªè¦ç‚¹ï¼‰\n",
    "    2. ä¿¡æ¯å¯ä¿¡åº¦è¯„ä¼°ï¼ˆ0-1è¯„åˆ†ï¼‰\n",
    "    3. ç ”ç©¶ç©ºç™½æˆ–éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥çš„é¢†åŸŸ\n",
    "    4. åˆæ­¥åˆ†ææ‘˜è¦\n",
    "\n",
    "    è¿”å›æ ¼å¼å¦‚ä¸‹ï¼š\n",
    "    å…³é”®å‘ç°ï¼š\n",
    "    - å‘ç°1\n",
    "    - å‘ç°2\n",
    "    - å‘ç°3\n",
    "\n",
    "    å¯ä¿¡åº¦è¯„ä¼°ï¼š\n",
    "    æ€»ä½“è¯„åˆ†ï¼šX.X\n",
    "    è¯¦ç»†è¯„ä¼°ï¼š...\n",
    "\n",
    "    ç ”ç©¶ç©ºç™½ï¼š\n",
    "    - ç©ºç™½1\n",
    "    - ç©ºç™½2\n",
    "\n",
    "    åˆ†ææ‘˜è¦ï¼š\n",
    "    [è¯¦ç»†çš„åˆ†ææ‘˜è¦]\n",
    "    \"\"\")\n",
    "\n",
    "    # å‡†å¤‡æœç´¢æ•°æ®\n",
    "    search_data = \"\"\n",
    "    for i, result in enumerate(state[\"search_results\"]):\n",
    "        search_data += f\"\\n=== æŸ¥è¯¢ {i+1}: {result['query']} ===\\n\"\n",
    "        if 'error' in result:\n",
    "            search_data += f\"é”™è¯¯ï¼š{result['error']}\\n\"\n",
    "        else:\n",
    "            search_data += f\"DuckDuckGoç»“æœï¼š{result.get('ddg_result', 'æ— ç»“æœ')}\\n\"\n",
    "            if result.get('wiki_result'):\n",
    "                search_data += f\"Wikipediaç»“æœï¼š{result['wiki_result']}\\n\"\n",
    "\n",
    "    # æ‰§è¡Œåˆ†æ\n",
    "    analysis_chain = analysis_prompt | llm\n",
    "    result = analysis_chain.invoke({\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"depth\": state[\"research_depth\"],\n",
    "        \"search_data\": search_data\n",
    "    })\n",
    "\n",
    "    # è§£æåˆ†æç»“æœ\n",
    "    analysis_text = result.content\n",
    "\n",
    "    # æå–å…³é”®å‘ç°\n",
    "    key_findings = []\n",
    "    if \"å…³é”®å‘ç°ï¼š\" in analysis_text:\n",
    "        findings_section = analysis_text.split(\"å…³é”®å‘ç°ï¼š\")[1].split(\"å¯ä¿¡åº¦è¯„ä¼°ï¼š\")[0]\n",
    "        for line in findings_section.split('\\n'):\n",
    "            if line.strip().startswith('-'):\n",
    "                key_findings.append(line.strip()[1:].strip())\n",
    "\n",
    "    # æå–ç ”ç©¶ç©ºç™½\n",
    "    research_gaps = []\n",
    "    if \"ç ”ç©¶ç©ºç™½ï¼š\" in analysis_text:\n",
    "        gaps_section = analysis_text.split(\"ç ”ç©¶ç©ºç™½ï¼š\")[1].split(\"åˆ†ææ‘˜è¦ï¼š\")[0]\n",
    "        for line in gaps_section.split('\\n'):\n",
    "            if line.strip().startswith('-'):\n",
    "                research_gaps.append(line.strip()[1:].strip())\n",
    "\n",
    "    # æå–åˆ†ææ‘˜è¦\n",
    "    analysis_summary = \"\"\n",
    "    if \"åˆ†ææ‘˜è¦ï¼š\" in analysis_text:\n",
    "        analysis_summary = analysis_text.split(\"åˆ†ææ‘˜è¦ï¼š\")[1].strip()\n",
    "\n",
    "    # ç®€å•çš„å¯ä¿¡åº¦è¯„åˆ†è®¡ç®—ï¼ˆåŸºäºç»“æœæ•°é‡å’Œè´¨é‡ï¼‰\n",
    "    successful_searches = len([r for r in state[\"search_results\"] if 'error' not in r])\n",
    "    credibility_score = min(successful_searches / len(state[\"search_results\"]), 1.0)\n",
    "\n",
    "    # æ›´æ–°çŠ¶æ€\n",
    "    state[\"key_findings\"] = key_findings[:5]  # é™åˆ¶ä¸º5ä¸ªå…³é”®å‘ç°\n",
    "    state[\"research_gaps\"] = research_gaps[:3]  # é™åˆ¶ä¸º3ä¸ªç ”ç©¶ç©ºç™½\n",
    "    state[\"analysis_summary\"] = analysis_summary\n",
    "    state[\"credibility_scores\"] = {\n",
    "        \"overall\": credibility_score,\n",
    "        \"source_diversity\": 0.8 if len(set(r.get('source_type', 'unknown') for r in state[\"search_results\"])) > 1 else 0.5,\n",
    "        \"information_depth\": 0.7 if len(analysis_summary) > 200 else 0.4\n",
    "    }\n",
    "    # print(state[\"credibility_scores\"])\n",
    "    \n",
    "    print(f\"âœ… åˆ†æå®Œæˆï¼Œè¯†åˆ«äº† {len(key_findings)} ä¸ªå…³é”®å‘ç°\")\n",
    "\n",
    "    return state\n",
    "\n",
    "def verify_quality(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    èŠ‚ç‚¹4ï¼šéªŒè¯è´¨é‡\n",
    "    åŠŸèƒ½ï¼šè¯„ä¼°ç ”ç©¶è´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦æ”¹è¿›\n",
    "    \"\"\"\n",
    "    print(\"âœ… æ­¥éª¤4ï¼šéªŒè¯è´¨é‡\")\n",
    "\n",
    "    state[\"current_step\"] = \"verification\"\n",
    "    state[\"completed_steps\"].append(\"verification\")\n",
    "    # print(state)\n",
    "    print(state[\"credibility_scores\"])\n",
    "\n",
    "    # è®¡ç®—è´¨é‡è¯„åˆ†\n",
    "    credibility_scores = state[\"credibility_scores\"]\n",
    "    quality_factors = [\n",
    "        credibility_scores.get(\"overall\", 0),\n",
    "        credibility_scores.get(\"source_diversity\", 0),\n",
    "        credibility_scores.get(\"information_depth\", 0),\n",
    "        len(state[\"key_findings\"]) / 5.0,  # æœŸæœ›è‡³å°‘5ä¸ªå‘ç°\n",
    "        len(state[\"research_notes\"]) / 10.0  # æœŸæœ›è‡³å°‘10ä¸ªç¬”è®°\n",
    "    ]\n",
    "\n",
    "    quality_score = sum(quality_factors) / len(quality_factors)\n",
    "    state[\"quality_score\"] = quality_score\n",
    "\n",
    "    # ç”Ÿæˆæ”¹è¿›å»ºè®®\n",
    "    improvement_suggestions = []\n",
    "\n",
    "    if quality_score < 0.7:\n",
    "        if credibility_scores.get(\"overall\", 0) < 0.6:\n",
    "            improvement_suggestions.append(\"å¢åŠ æ›´å¤šå¯é çš„ä¿¡æ¯æº\")\n",
    "        if len(state[\"key_findings\"]) < 3:\n",
    "            improvement_suggestions.append(\"æ‰©å±•æœç´¢èŒƒå›´ä»¥è·å¾—æ›´å¤šå…³é”®å‘ç°\")\n",
    "        if len(state[\"research_notes\"]) < 5:\n",
    "            improvement_suggestions.append(\"æ·±å…¥ç ”ç©¶æœç´¢ç»“æœçš„ç»†èŠ‚\")\n",
    "\n",
    "        state[\"improvement_suggestions\"] = improvement_suggestions\n",
    "        state[\"verification_status\"] = False\n",
    "        print(f\"âš ï¸  è´¨é‡è¯„åˆ† {quality_score:.2f} ä½äºé˜ˆå€¼ï¼Œéœ€è¦æ”¹è¿›\")\n",
    "    else:\n",
    "        state[\"verification_status\"] = True\n",
    "        print(f\"âœ… è´¨é‡è¯„åˆ† {quality_score:.2f} é€šè¿‡éªŒè¯\")\n",
    "\n",
    "    return state\n",
    "\n",
    "def generate_report(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    èŠ‚ç‚¹5ï¼šç”ŸæˆæŠ¥å‘Š\n",
    "    åŠŸèƒ½ï¼šåŸºäºåˆ†æç»“æœç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Š\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“ æ­¥éª¤5ï¼šç”ŸæˆæŠ¥å‘Š\")\n",
    "\n",
    "    state[\"current_step\"] = \"reporting\"\n",
    "    state[\"completed_steps\"].append(\"reporting\")\n",
    "\n",
    "    # ç”ŸæˆæŠ¥å‘Šçš„æç¤º\n",
    "    report_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    åŸºäºä»¥ä¸‹ç ”ç©¶æ•°æ®ç”Ÿæˆä¸“ä¸šçš„ç ”ç©¶æŠ¥å‘Šï¼š\n",
    "\n",
    "    ç ”ç©¶ä¸»é¢˜ï¼š{topic}\n",
    "    ç ”ç©¶æ·±åº¦ï¼š{depth}\n",
    "\n",
    "    å…³é”®å‘ç°ï¼š\n",
    "    {key_findings}\n",
    "\n",
    "    åˆ†ææ‘˜è¦ï¼š\n",
    "    {analysis_summary}\n",
    "\n",
    "    ç ”ç©¶ç©ºç™½ï¼š\n",
    "    {research_gaps}\n",
    "\n",
    "    è´¨é‡è¯„åˆ†ï¼š{quality_score:.2f}\n",
    "\n",
    "    è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹ç»“æ„çš„æŠ¥å‘Šï¼š\n",
    "\n",
    "    # æ‰§è¡Œæ‘˜è¦\n",
    "    [ç®€æ˜æ‰¼è¦çš„æ¦‚è¿°ï¼Œ200-300å­—]\n",
    "\n",
    "    # ç ”ç©¶æ–¹æ³•\n",
    "    [è¯´æ˜ç ”ç©¶æ–¹æ³•å’Œæ•°æ®æ¥æº]\n",
    "\n",
    "    # ä¸»è¦å‘ç°\n",
    "    [è¯¦ç»†å±•å¼€å…³é”®å‘ç°]\n",
    "\n",
    "    # åˆ†æä¸è®¨è®º\n",
    "    [æ·±å…¥åˆ†æå’Œè®¨è®º]\n",
    "\n",
    "    # ç ”ç©¶å±€é™\n",
    "    [è¯´æ˜ç ”ç©¶çš„å±€é™æ€§]\n",
    "\n",
    "    # ç»“è®ºä¸å»ºè®®\n",
    "    [æ€»ç»“ç»“è®ºå’Œå»ºè®®]\n",
    "\n",
    "    # é™„å½•\n",
    "    [æ•°æ®æ¥æºå’Œå‚è€ƒæ–‡çŒ®]\n",
    "    \"\"\")\n",
    "\n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    key_findings_text = \"\\n\".join([f\"- {finding}\" for finding in state[\"key_findings\"]])\n",
    "    research_gaps_text = \"\\n\".join([f\"- {gap}\" for gap in state[\"research_gaps\"]])\n",
    "\n",
    "    # ç”ŸæˆæŠ¥å‘Š\n",
    "    report_chain = report_prompt | llm\n",
    "    result = report_chain.invoke({\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"depth\": state[\"research_depth\"],\n",
    "        \"key_findings\": key_findings_text,\n",
    "        \"analysis_summary\": state[\"analysis_summary\"],\n",
    "        \"research_gaps\": research_gaps_text,\n",
    "        \"quality_score\": state[\"quality_score\"]\n",
    "    })\n",
    "\n",
    "    # æå–æ‰§è¡Œæ‘˜è¦\n",
    "    report_content = result.content\n",
    "    executive_summary = \"\"\n",
    "    if \"# æ‰§è¡Œæ‘˜è¦\" in report_content:\n",
    "        summary_section = report_content.split(\"# æ‰§è¡Œæ‘˜è¦\")[1].split(\"#\")[0]\n",
    "        executive_summary = summary_section.strip()\n",
    "\n",
    "    # æ›´æ–°çŠ¶æ€\n",
    "    state[\"final_report\"] = report_content\n",
    "    state[\"executive_summary\"] = executive_summary\n",
    "\n",
    "    print(\"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ\")\n",
    "\n",
    "    return state\n",
    "\n",
    "def improve_research(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    èŠ‚ç‚¹6ï¼šæ”¹è¿›ç ”ç©¶\n",
    "    åŠŸèƒ½ï¼šåŸºäºè´¨é‡éªŒè¯ç»“æœï¼Œæ”¹è¿›ç ”ç©¶è´¨é‡\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”§ æ­¥éª¤6ï¼šæ”¹è¿›ç ”ç©¶\")\n",
    "\n",
    "    state[\"current_step\"] = \"improvement\"\n",
    "    state[\"completed_steps\"].append(\"improvement\")\n",
    "    state[\"retry_count\"] += 1\n",
    "\n",
    "    # åŸºäºæ”¹è¿›å»ºè®®ç”Ÿæˆæ–°çš„æœç´¢æŸ¥è¯¢\n",
    "    suggestions = state[\"improvement_suggestions\"]\n",
    "\n",
    "    improvement_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    åŸç ”ç©¶ä¸»é¢˜ï¼š{topic}\n",
    "\n",
    "    å½“å‰ç ”ç©¶çš„ä¸è¶³ï¼š\n",
    "    {suggestions}\n",
    "\n",
    "    å·²å°è¯•çš„æœç´¢æŸ¥è¯¢ï¼š\n",
    "    {existing_queries}\n",
    "\n",
    "    ä¸ºäº†æ”¹è¿›ç ”ç©¶è´¨é‡ï¼Œè¯·ç”Ÿæˆ3ä¸ªæ–°çš„æœç´¢æŸ¥è¯¢ï¼Œä¸“æ³¨äºï¼š\n",
    "    1. æ›´å¯é çš„ä¿¡æ¯æº\n",
    "    2. æ›´æ·±å…¥çš„æŠ€æœ¯ç»†èŠ‚\n",
    "    3. ä¸åŒçš„è§’åº¦å’Œè§‚ç‚¹\n",
    "\n",
    "    è¿”å›æ–°çš„æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªï¼š\n",
    "    \"\"\")\n",
    "\n",
    "    existing_queries = \"\\n\".join(state[\"search_queries\"])\n",
    "    suggestions_text = \"\\n\".join([f\"- {suggestion}\" for suggestion in suggestions])\n",
    "\n",
    "    improvement_chain = improvement_prompt | llm\n",
    "    result = improvement_chain.invoke({\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"suggestions\": suggestions_text,\n",
    "        \"existing_queries\": existing_queries\n",
    "    })\n",
    "\n",
    "    # è§£ææ–°æŸ¥è¯¢å¹¶æ·»åŠ åˆ°ç°æœ‰æŸ¥è¯¢ä¸­\n",
    "    new_queries = [q.strip() for q in result.content.split('\\n') if q.strip()]\n",
    "    state[\"search_queries\"].extend(new_queries[:3])\n",
    "\n",
    "    print(f\"âœ… æ·»åŠ äº† {len(new_queries)} ä¸ªæ–°çš„æœç´¢æŸ¥è¯¢\")\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f6ab46-7d20-42d5-9f6e-32b85e8f0091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T12:34:07.858097Z",
     "iopub.status.busy": "2025-11-11T12:34:07.857867Z",
     "iopub.status.idle": "2025-11-11T12:34:07.861249Z",
     "shell.execute_reply": "2025-11-11T12:34:07.860850Z",
     "shell.execute_reply.started": "2025-11-11T12:34:07.858081Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. æ¡ä»¶è·¯ç”±å‡½æ•° (Conditional Routing Functions)\n",
    "# ============================================================================\n",
    "\n",
    "def should_improve_or_continue(state: ResearchState) -> str:\n",
    "    \"\"\"\n",
    "    æ¡ä»¶è·¯ç”±ï¼šå†³å®šæ˜¯å¦éœ€è¦æ”¹è¿›ç ”ç©¶æˆ–ç»§ç»­ç”ŸæˆæŠ¥å‘Š\n",
    "    \"\"\"\n",
    "    if not state[\"verification_status\"] and state[\"retry_count\"] < state[\"max_retries\"]:\n",
    "        return \"improve\"\n",
    "    else:\n",
    "        return \"generate_report\"\n",
    "\n",
    "def should_retry_or_finish(state: ResearchState) -> str:\n",
    "    \"\"\"\n",
    "    æ¡ä»¶è·¯ç”±ï¼šå†³å®šæ˜¯å¦é‡è¯•æœç´¢æˆ–ç»“æŸæµç¨‹\n",
    "    \"\"\"\n",
    "    if state[\"retry_count\"] >= state[\"max_retries\"]:\n",
    "        return \"generate_report\"\n",
    "    else:\n",
    "        return \"search\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e81e1e-5049-40dc-ab11-21aefadf4b25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T12:34:11.424107Z",
     "iopub.status.busy": "2025-11-11T12:34:11.423855Z",
     "iopub.status.idle": "2025-11-11T12:34:11.428318Z",
     "shell.execute_reply": "2025-11-11T12:34:11.427920Z",
     "shell.execute_reply.started": "2025-11-11T12:34:11.424089Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. åˆ›å»ºå·¥ä½œæµå›¾ (Create Workflow Graph)\n",
    "# ============================================================================\n",
    "\n",
    "def create_research_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    åˆ›å»ºå®Œæ•´çš„ç ”ç©¶å·¥ä½œæµå›¾\n",
    "    \"\"\"\n",
    "    # åˆ›å»ºçŠ¶æ€å›¾\n",
    "    workflow = StateGraph(ResearchState)\n",
    "\n",
    "    # æ·»åŠ èŠ‚ç‚¹\n",
    "    workflow.add_node(\"initialize\", initialize_research)\n",
    "    workflow.add_node(\"search\", conduct_search)\n",
    "    workflow.add_node(\"analyze\", analyze_findings)\n",
    "    workflow.add_node(\"verify\", verify_quality)\n",
    "    workflow.add_node(\"generate_report\", generate_report)\n",
    "    workflow.add_node(\"improve\", improve_research)\n",
    "\n",
    "    # æ·»åŠ è¾¹ï¼ˆå®šä¹‰å·¥ä½œæµè·¯å¾„ï¼‰\n",
    "    workflow.add_edge(START, \"initialize\")           # å¼€å§‹ -> åˆå§‹åŒ–\n",
    "    workflow.add_edge(\"initialize\", \"search\")         # åˆå§‹åŒ– -> æœç´¢\n",
    "    workflow.add_edge(\"search\", \"analyze\")            # æœç´¢ -> åˆ†æ\n",
    "    workflow.add_edge(\"analyze\", \"verify\")            # åˆ†æ -> éªŒè¯\n",
    "\n",
    "    # æ¡ä»¶è¾¹ï¼šéªŒè¯åçš„åˆ†æ”¯\n",
    "    workflow.add_conditional_edges(\n",
    "        \"verify\",\n",
    "        should_improve_or_continue,\n",
    "        {\n",
    "            \"improve\": \"improve\",           # éœ€è¦æ”¹è¿›\n",
    "            \"generate_report\": \"generate_report\"  # ç»§ç»­ç”ŸæˆæŠ¥å‘Š\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # æ”¹è¿›åå›åˆ°æœç´¢\n",
    "    workflow.add_edge(\"improve\", \"search\")\n",
    "\n",
    "    # ç»“æŸæµç¨‹\n",
    "    workflow.add_edge(\"generate_report\", END)\n",
    "\n",
    "    return workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83f44306-1eeb-4249-9f91-de87fab995ae",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-11T12:34:14.135546Z",
     "iopub.status.busy": "2025-11-11T12:34:14.135309Z",
     "iopub.status.idle": "2025-11-11T12:34:14.143506Z",
     "shell.execute_reply": "2025-11-11T12:34:14.143093Z",
     "shell.execute_reply.started": "2025-11-11T12:34:14.135529Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. ä¸»å‡½æ•°å’Œç¤ºä¾‹ä½¿ç”¨ (Main Function and Usage Examples)\n",
    "# ============================================================================\n",
    "\n",
    "def run_research_assistant(topic: str, research_depth: str = \"standard\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    è¿è¡Œç ”ç©¶åŠ©æ‰‹\n",
    "\n",
    "    Args:\n",
    "        topic: ç ”ç©¶ä¸»é¢˜\n",
    "        research_depth: ç ”ç©¶æ·±åº¦ (basic/standard/comprehensive)\n",
    "\n",
    "    Returns:\n",
    "        åŒ…å«ç ”ç©¶ç»“æœçš„å­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ¯ å¼€å§‹ç ”ç©¶ä¸»é¢˜ï¼š{topic}\")\n",
    "    print(f\"ğŸ“Š ç ”ç©¶æ·±åº¦ï¼š{research_depth}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # åˆ›å»ºå·¥ä½œæµ\n",
    "    research_workflow = create_research_graph()\n",
    "\n",
    "    # ç¼–è¯‘å·¥ä½œæµï¼ˆæ·»åŠ å†…å­˜ä¿å­˜å™¨ä»¥æ”¯æŒæ£€æŸ¥ç‚¹ï¼‰\n",
    "    app = research_workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "    # åˆå§‹åŒ–çŠ¶æ€\n",
    "    initial_state = ResearchState(\n",
    "        topic=topic,\n",
    "        research_depth=research_depth,\n",
    "        search_queries=[],\n",
    "        search_results=[],\n",
    "        research_notes=[],\n",
    "        key_findings=[],\n",
    "        analysis_summary=\"\",\n",
    "        credibility_scores={},\n",
    "        research_gaps=[],\n",
    "        verification_status=False,\n",
    "        quality_score=0.0,\n",
    "        improvement_suggestions=[],\n",
    "        final_report=\"\",\n",
    "        executive_summary=\"\",\n",
    "        current_step=\"\",\n",
    "        retry_count=0,\n",
    "        max_retries=2,\n",
    "        completed_steps=[],\n",
    "        messages=[]\n",
    "    )\n",
    "\n",
    "    # æ‰§è¡Œå·¥ä½œæµ\n",
    "    try:\n",
    "        # final_state = app.invoke(initial_state)\n",
    "        # æä¾›é…ç½®å‚æ•°ä»¥æ”¯æŒæ£€æŸ¥ç‚¹\n",
    "        config = {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": f\"research_{topic}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            }\n",
    "        }\n",
    "        final_state = app.invoke(initial_state, config=config)\n",
    "        \n",
    "        # è¾“å‡ºç»“æœæ‘˜è¦\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ğŸ“‹ ç ”ç©¶å®Œæˆæ‘˜è¦\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"ğŸ¯ ç ”ç©¶ä¸»é¢˜ï¼š{final_state['topic']}\")\n",
    "        print(f\"ğŸ“Š è´¨é‡è¯„åˆ†ï¼š{final_state['quality_score']:.2f}\")\n",
    "        print(f\"ğŸ” æ‰§è¡Œæœç´¢ï¼š{len(final_state['search_results'])} æ¬¡\")\n",
    "        print(f\"ğŸ’¡ å…³é”®å‘ç°ï¼š{len(final_state['key_findings'])} ä¸ª\")\n",
    "        print(f\"ğŸ“ å®Œæˆæ­¥éª¤ï¼š{', '.join(final_state['completed_steps'])}\")\n",
    "\n",
    "        if final_state['executive_summary']:\n",
    "            print(f\"\\nğŸ“„ æ‰§è¡Œæ‘˜è¦ï¼š\")\n",
    "            print(\"-\" * 30)\n",
    "            print(final_state['executive_summary'][:300] + \"...\" if len(final_state['executive_summary']) > 300 else final_state['executive_summary'])\n",
    "\n",
    "        return final_state\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def save_results_to_file(results: Dict[str, Any], filename: str = None) -> str:\n",
    "    \"\"\"\n",
    "    ä¿å­˜ç ”ç©¶ç»“æœåˆ°æ–‡ä»¶\n",
    "\n",
    "    Args:\n",
    "        results: ç ”ç©¶ç»“æœ\n",
    "        filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "    Returns:\n",
    "        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"research_report_{timestamp}.md\"\n",
    "\n",
    "    # åˆ›å»ºæŠ¥å‘Šå†…å®¹\n",
    "    report_content = results.get('final_report', 'æ— æŠ¥å‘Šå†…å®¹')\n",
    "\n",
    "    # æ·»åŠ å…ƒæ•°æ®\n",
    "    metadata = f\"\"\"---\n",
    "ç ”ç©¶ä¸»é¢˜ï¼š{results.get('topic', 'æœªçŸ¥')}\n",
    "ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "è´¨é‡è¯„åˆ†ï¼š{results.get('quality_score', 0):.2f}\n",
    "å®Œæˆæ­¥éª¤ï¼š{', '.join(results.get('completed_steps', []))}\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    full_content = metadata + report_content\n",
    "\n",
    "    # ä¿å­˜æ–‡ä»¶\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(full_content)\n",
    "\n",
    "    print(f\"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°ï¼š{filename}\")\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13ae6576-464a-4066-99f4-90fdfb55ab06",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-11T13:15:32.408100Z",
     "iopub.status.busy": "2025-11-11T13:15:32.407882Z",
     "iopub.status.idle": "2025-11-11T13:22:17.614210Z",
     "shell.execute_reply": "2025-11-11T13:22:17.613762Z",
     "shell.execute_reply.started": "2025-11-11T13:15:32.408086Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸš€ ç¤ºä¾‹2ï¼šæ·±å…¥ç ”ç©¶\n",
      "ğŸ¯ å¼€å§‹ç ”ç©¶ä¸»é¢˜ï¼šé‡å­è®¡ç®—å¯¹å¯†ç å­¦çš„å½±å“\n",
      "ğŸ“Š ç ”ç©¶æ·±åº¦ï¼šcomprehensive\n",
      "==================================================\n",
      "ğŸš€ æ­¥éª¤1ï¼šåˆå§‹åŒ–ç ”ç©¶\n",
      "âœ… ç”Ÿæˆäº† 8 ä¸ªæœç´¢æŸ¥è¯¢\n",
      " æ­¥éª¤2: Conducting search\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 1/8: 1. é‡å­è®¡ç®—åŸç†ä¸å¯†ç å­¦åŸºç¡€ï¼šShorç®—æ³•ã€Groverç®—æ³•å¯¹åŠ å¯†ä½“ç³»çš„å½±å“æœºåˆ¶\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 2/8: 2. åé‡å­å¯†ç å­¦æœ€æ–°ç ”ç©¶è¿›å±•ï¼šNISTæ ‡å‡†åŒ–è¿›ç¨‹ä¸é‡å­å®‰å…¨åŠ å¯†ç®—æ³•å¼€å‘åŠ¨æ€\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 3/8: 3. é‡å­å¯†ç å­¦é¢†åŸŸæ ¸å¿ƒç ”ç©¶æœºæ„ä¸å­¦è€…ï¼šMITé‡å­è®¡ç®—ä¸­å¿ƒã€æ»‘é“å¢å¤§å­¦IQCç ”ç©¶æˆæœåˆ†æ\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 4/8: 4. é‡å­éœ¸æƒäº‰è®®ä¸å¯†ç å­¦å®è·µæŒ‘æˆ˜ï¼šå®é™…é‡å­è®¡ç®—æœºç ´è§£RSAåŠ å¯†çš„æŠ€æœ¯éšœç¢\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 5/8: 5. é‡å­ç½‘ç»œä¸å¯†ç å­¦æœªæ¥å±•æœ›ï¼šé‡å­äº’è”ç½‘æ¶æ„ä¸‹çš„å…¨çƒå®‰å…¨é€šä¿¡ä½“ç³»æ„æƒ³\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 6/8: 6. æŠ—é‡å­å¯†ç ç®—æ³•è¿ç§»æˆæœ¬ä¸ä¼ä¸šå®æ–½è·¯å¾„ï¼šé‡‘èã€æ”¿åŠ¡é¢†åŸŸå¯†ç ç³»ç»Ÿå‡çº§æ–¹æ¡ˆ\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 7/8: 7. é‡å­å¯†ç ä¸ç»å…¸å¯†ç èåˆç ”ç©¶ï¼šQKDä¸ä¼ ç»ŸåŠ å¯†çš„ååŒå®‰å…¨æ¨¡å‹æ¯”è¾ƒ\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 8/8: 8. é‡å­è®¡ç®—å‘å±•æ—¶é—´çº¿é¢„æµ‹ä¸å¯†ç å­¦åº”å¯¹ç­–ç•¥ï¼š2030å¹´å‰ååŠ å¯†ç³»ç»Ÿæ¼”è¿›è·¯çº¿å›¾\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "[OK] Completed 8 search queries\n",
      "ğŸ“Š æ­¥éª¤3ï¼šåˆ†æå‘ç°\n",
      "âœ… åˆ†æå®Œæˆï¼Œè¯†åˆ«äº† 5 ä¸ªå…³é”®å‘ç°\n",
      "âœ… æ­¥éª¤4ï¼šéªŒè¯è´¨é‡\n",
      "{'overall': 0.0, 'source_diversity': 0.5, 'information_depth': 0.7}\n",
      "âš ï¸  è´¨é‡è¯„åˆ† 0.44 ä½äºé˜ˆå€¼ï¼Œéœ€è¦æ”¹è¿›\n",
      "ğŸ”§ æ­¥éª¤6ï¼šæ”¹è¿›ç ”ç©¶\n",
      "âœ… æ·»åŠ äº† 3 ä¸ªæ–°çš„æœç´¢æŸ¥è¯¢\n",
      " æ­¥éª¤2: Conducting search\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 1/11: 1. é‡å­è®¡ç®—åŸç†ä¸å¯†ç å­¦åŸºç¡€ï¼šShorç®—æ³•ã€Groverç®—æ³•å¯¹åŠ å¯†ä½“ç³»çš„å½±å“æœºåˆ¶\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 2/11: 2. åé‡å­å¯†ç å­¦æœ€æ–°ç ”ç©¶è¿›å±•ï¼šNISTæ ‡å‡†åŒ–è¿›ç¨‹ä¸é‡å­å®‰å…¨åŠ å¯†ç®—æ³•å¼€å‘åŠ¨æ€\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 3/11: 3. é‡å­å¯†ç å­¦é¢†åŸŸæ ¸å¿ƒç ”ç©¶æœºæ„ä¸å­¦è€…ï¼šMITé‡å­è®¡ç®—ä¸­å¿ƒã€æ»‘é“å¢å¤§å­¦IQCç ”ç©¶æˆæœåˆ†æ\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 4/11: 4. é‡å­éœ¸æƒäº‰è®®ä¸å¯†ç å­¦å®è·µæŒ‘æˆ˜ï¼šå®é™…é‡å­è®¡ç®—æœºç ´è§£RSAåŠ å¯†çš„æŠ€æœ¯éšœç¢\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 5/11: 5. é‡å­ç½‘ç»œä¸å¯†ç å­¦æœªæ¥å±•æœ›ï¼šé‡å­äº’è”ç½‘æ¶æ„ä¸‹çš„å…¨çƒå®‰å…¨é€šä¿¡ä½“ç³»æ„æƒ³\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 6/11: 6. æŠ—é‡å­å¯†ç ç®—æ³•è¿ç§»æˆæœ¬ä¸ä¼ä¸šå®æ–½è·¯å¾„ï¼šé‡‘èã€æ”¿åŠ¡é¢†åŸŸå¯†ç ç³»ç»Ÿå‡çº§æ–¹æ¡ˆ\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 7/11: 7. é‡å­å¯†ç ä¸ç»å…¸å¯†ç èåˆç ”ç©¶ï¼šQKDä¸ä¼ ç»ŸåŠ å¯†çš„ååŒå®‰å…¨æ¨¡å‹æ¯”è¾ƒ\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 8/11: 8. é‡å­è®¡ç®—å‘å±•æ—¶é—´çº¿é¢„æµ‹ä¸å¯†ç å­¦åº”å¯¹ç­–ç•¥ï¼š2030å¹´å‰ååŠ å¯†ç³»ç»Ÿæ¼”è¿›è·¯çº¿å›¾\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 9/11: 1. NIST IR 8105 ä¸ NIST SP 800-208 æŠ€æœ¯ç»†èŠ‚åˆ†æï¼šåé‡å­å¯†ç æ ‡å‡†ç®—æ³•çš„æ•°å­¦åŸç†ä¸å®ç°å¤æ‚åº¦è¯„ä¼°\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 10/11: 2. é‡å­å¯†é’¥åˆ†å‘å®é™…éƒ¨ç½²ä¸­çš„ä¾§ä¿¡é“æ”»å‡»ä¸ç‰©ç†å±‚å®‰å…¨æ¼æ´ï¼šåŸºäºBB84åè®®çš„é‡å­é»‘å®¢æ”»å‡»æ¡ˆä¾‹ç ”ç©¶\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 11/11: 3. é‡å­è®¡ç®—å‘å±•è·¯å¾„çš„å­¦æœ¯äº‰è®®ï¼šIBMä¸Googleåœ¨é‡å­çº é”™é˜ˆå€¼å®ç°æ—¶é—´çº¿ä¸Šçš„åˆ†æ­§åŠå…¶å¯¹å¯†ç è¿ç§»ç­–ç•¥çš„å½±å“\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "[OK] Completed 11 search queries\n",
      "ğŸ“Š æ­¥éª¤3ï¼šåˆ†æå‘ç°\n",
      "âœ… åˆ†æå®Œæˆï¼Œè¯†åˆ«äº† 5 ä¸ªå…³é”®å‘ç°\n",
      "âœ… æ­¥éª¤4ï¼šéªŒè¯è´¨é‡\n",
      "{'overall': 0.0, 'source_diversity': 0.5, 'information_depth': 0.7}\n",
      "âš ï¸  è´¨é‡è¯„åˆ† 0.44 ä½äºé˜ˆå€¼ï¼Œéœ€è¦æ”¹è¿›\n",
      "ğŸ”§ æ­¥éª¤6ï¼šæ”¹è¿›ç ”ç©¶\n",
      "âœ… æ·»åŠ äº† 3 ä¸ªæ–°çš„æœç´¢æŸ¥è¯¢\n",
      " æ­¥éª¤2: Conducting search\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 1/14: 1. é‡å­è®¡ç®—åŸç†ä¸å¯†ç å­¦åŸºç¡€ï¼šShorç®—æ³•ã€Groverç®—æ³•å¯¹åŠ å¯†ä½“ç³»çš„å½±å“æœºåˆ¶\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 2/14: 2. åé‡å­å¯†ç å­¦æœ€æ–°ç ”ç©¶è¿›å±•ï¼šNISTæ ‡å‡†åŒ–è¿›ç¨‹ä¸é‡å­å®‰å…¨åŠ å¯†ç®—æ³•å¼€å‘åŠ¨æ€\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 3/14: 3. é‡å­å¯†ç å­¦é¢†åŸŸæ ¸å¿ƒç ”ç©¶æœºæ„ä¸å­¦è€…ï¼šMITé‡å­è®¡ç®—ä¸­å¿ƒã€æ»‘é“å¢å¤§å­¦IQCç ”ç©¶æˆæœåˆ†æ\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 4/14: 4. é‡å­éœ¸æƒäº‰è®®ä¸å¯†ç å­¦å®è·µæŒ‘æˆ˜ï¼šå®é™…é‡å­è®¡ç®—æœºç ´è§£RSAåŠ å¯†çš„æŠ€æœ¯éšœç¢\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 5/14: 5. é‡å­ç½‘ç»œä¸å¯†ç å­¦æœªæ¥å±•æœ›ï¼šé‡å­äº’è”ç½‘æ¶æ„ä¸‹çš„å…¨çƒå®‰å…¨é€šä¿¡ä½“ç³»æ„æƒ³\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 6/14: 6. æŠ—é‡å­å¯†ç ç®—æ³•è¿ç§»æˆæœ¬ä¸ä¼ä¸šå®æ–½è·¯å¾„ï¼šé‡‘èã€æ”¿åŠ¡é¢†åŸŸå¯†ç ç³»ç»Ÿå‡çº§æ–¹æ¡ˆ\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 7/14: 7. é‡å­å¯†ç ä¸ç»å…¸å¯†ç èåˆç ”ç©¶ï¼šQKDä¸ä¼ ç»ŸåŠ å¯†çš„ååŒå®‰å…¨æ¨¡å‹æ¯”è¾ƒ\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 8/14: 8. é‡å­è®¡ç®—å‘å±•æ—¶é—´çº¿é¢„æµ‹ä¸å¯†ç å­¦åº”å¯¹ç­–ç•¥ï¼š2030å¹´å‰ååŠ å¯†ç³»ç»Ÿæ¼”è¿›è·¯çº¿å›¾\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 9/14: 1. NIST IR 8105 ä¸ NIST SP 800-208 æŠ€æœ¯ç»†èŠ‚åˆ†æï¼šåé‡å­å¯†ç æ ‡å‡†ç®—æ³•çš„æ•°å­¦åŸç†ä¸å®ç°å¤æ‚åº¦è¯„ä¼°\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 10/14: 2. é‡å­å¯†é’¥åˆ†å‘å®é™…éƒ¨ç½²ä¸­çš„ä¾§ä¿¡é“æ”»å‡»ä¸ç‰©ç†å±‚å®‰å…¨æ¼æ´ï¼šåŸºäºBB84åè®®çš„é‡å­é»‘å®¢æ”»å‡»æ¡ˆä¾‹ç ”ç©¶\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 11/14: 3. é‡å­è®¡ç®—å‘å±•è·¯å¾„çš„å­¦æœ¯äº‰è®®ï¼šIBMä¸Googleåœ¨é‡å­çº é”™é˜ˆå€¼å®ç°æ—¶é—´çº¿ä¸Šçš„åˆ†æ­§åŠå…¶å¯¹å¯†ç è¿ç§»ç­–ç•¥çš„å½±å“\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 12/14: 1. NISTåé‡å­å¯†ç æ ‡å‡†åŒ–é¡¹ç›®æŠ€æœ¯æŠ¥å‘Šä¸å…¬å¼€è¯„ä¼°æ–‡æ¡£ï¼šFIPS 203 (ML-KEM)ã€204 (ML-DSA)ã€205 (ML-KEA)è§„èŒƒåŠç¬¬ä¸‰è½®å€™é€‰ç®—æ³•å®‰å…¨æ€§åˆ†æ\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 13/14: 2. é‡å­èµ„æºä¼°ç®—æ¡†æ¶ä¸‹çš„å¯†ç åˆ†æå®è·µï¼šConcreteé‡å­é—¨å¤æ‚åº¦åˆ†æå·¥å…·åœ¨AESã€RSAç ´è§£è·¯å¾„æ¨¡æ‹Ÿä¸­çš„åº”ç”¨ä¸å±€é™æ€§\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "  æ‰§è¡ŒæŸ¥è¯¢ 14/14: 3. æ¬§ç›ŸPQCIRCUSé¡¹ç›®ä¸ç¾å›½QEDé¡¹ç›®æ¯”è¾ƒç ”ç©¶ï¼šä¸åŒæŠ€æœ¯è·¯çº¿ä¸‹æ··åˆå¯†ç ç³»ç»Ÿ(åé‡å­å¯†ç +é‡å­å¯†é’¥åˆ†å‘)çš„æ ‡å‡†åŒ–ç­–ç•¥å·®å¼‚ä¸äº§ä¸šå½±å“\n",
      "    [FAIL] Search failed: name 'wikipedia_tool' is not defined\n",
      "[OK] Completed 14 search queries\n",
      "ğŸ“Š æ­¥éª¤3ï¼šåˆ†æå‘ç°\n",
      "âœ… åˆ†æå®Œæˆï¼Œè¯†åˆ«äº† 4 ä¸ªå…³é”®å‘ç°\n",
      "âœ… æ­¥éª¤4ï¼šéªŒè¯è´¨é‡\n",
      "{'overall': 0.0, 'source_diversity': 0.5, 'information_depth': 0.7}\n",
      "âš ï¸  è´¨é‡è¯„åˆ† 0.40 ä½äºé˜ˆå€¼ï¼Œéœ€è¦æ”¹è¿›\n",
      "ğŸ“ æ­¥éª¤5ï¼šç”ŸæˆæŠ¥å‘Š\n",
      "âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ\n",
      "\n",
      "==================================================\n",
      "ğŸ“‹ ç ”ç©¶å®Œæˆæ‘˜è¦\n",
      "==================================================\n",
      "ğŸ¯ ç ”ç©¶ä¸»é¢˜ï¼šé‡å­è®¡ç®—å¯¹å¯†ç å­¦çš„å½±å“\n",
      "ğŸ“Š è´¨é‡è¯„åˆ†ï¼š0.40\n",
      "ğŸ” æ‰§è¡Œæœç´¢ï¼š14 æ¬¡\n",
      "ğŸ’¡ å…³é”®å‘ç°ï¼š4 ä¸ª\n",
      "ğŸ“ å®Œæˆæ­¥éª¤ï¼šinitialization, searching, analysis, verification, improvement, searching, analysis, verification, improvement, searching, analysis, verification, reporting\n",
      "\n",
      "ğŸ“„ æ‰§è¡Œæ‘˜è¦ï¼š\n",
      "------------------------------\n",
      "æœ¬ç ”ç©¶æŠ¥å‘Šå…¨é¢åˆ†æäº†é‡å­è®¡ç®—å¯¹ç°ä»£å¯†ç å­¦ä½“ç³»çš„å¤šç»´åº¦å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé‡å­è®¡ç®—æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯Shorç®—æ³•å’ŒGroverç®—æ³•ï¼Œå¯¹ç°æœ‰åŠ å¯†ä½“ç³»æ„æˆç³»ç»Ÿæ€§å¨èƒï¼Œå…¶ä¸­RSAã€ECCç­‰å…¬é’¥å¯†ç ä½“åˆ¶é¢ä¸´è¢«ç›´æ¥ç ´è§£çš„é£é™©ï¼Œè€Œå¯¹ç§°å¯†ç çš„å¯†é’¥å¼ºåº¦éœ€æ±‚å°†æ˜¾è‘—å¢åŠ ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œåé‡å­å¯†ç å­¦å’Œé‡å­å¯†ç å­¦å·²å‘å±•æˆä¸ºä¸¤å¤§ä¸»è¦æŠ€æœ¯è·¯å¾„ï¼šå‰è€…é€šè¿‡æ•°å­¦å±‚é¢çš„ç®—æ³•æ›¿ä»£æä¾›é‡å­å®‰å…¨æ€§ï¼Œåè€…åˆ™åŸºäºé‡å­ç‰©ç†åŸç†å®ç°å®‰å…¨ä¿éšœã€‚ç›®å‰ï¼Œå…¨çƒæ ‡å‡†åŒ–è¿›ç¨‹å·²è¿›å…¥å®è´¨é˜¶æ®µï¼Œç¾å›½å›½å®¶æ ‡å‡†ä¸æŠ€æœ¯ç ”ç©¶é™¢ï¼ˆNISTï¼‰ç­‰æœºæ„æ­£ç§¯ææ¨åŠ¨åé‡å­å¯†ç æ ‡å‡†çš„åˆ¶å®šä¸å®æ–½ï¼Œå·²åˆæ­¥ç¡®å®šML-KEMã€ML-DSAç­‰æ ‡å‡†ç®—æ³•ã€‚äº§ä¸šç•Œæ­£åœ¨æ¢ç´¢åŒ…æ‹¬ç®—æ³•è¿ç§»ã€æ··åˆæ–¹æ¡ˆéƒ¨ç½²ç­‰åº”å¯¹ç­–ç•¥...\n",
      "ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°ï¼šquantum_cryptography_comprehensive.md\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. ç¤ºä¾‹ä½¿ç”¨ (Usage Examples)\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     # ç¤ºä¾‹1ï¼šåŸºç¡€ç ”ç©¶\n",
    "#     print(\"ğŸš€ ç¤ºä¾‹1ï¼šåŸºç¡€ç ”ç©¶\")\n",
    "#     results1 = run_research_assistant(\n",
    "#         topic=\"LLMæœ€æ–°è¿›å±•ï¼ŒåŠåœ¨ç¼–ç¨‹æ–¹é¢çš„åº”ç”¨\",\n",
    "#         research_depth=\"basic\"\n",
    "#     )\n",
    "#     save_results_to_file(results1, \"ai_medical_diagnosis_basic.md\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "    # ç¤ºä¾‹2ï¼šæ·±å…¥ç ”ç©¶\n",
    "    print(\"ğŸš€ ç¤ºä¾‹2ï¼šæ·±å…¥ç ”ç©¶\")\n",
    "    results2 = run_research_assistant(\n",
    "        topic=\"é‡å­è®¡ç®—å¯¹å¯†ç å­¦çš„å½±å“\",\n",
    "        research_depth=\"comprehensive\"\n",
    "    )\n",
    "    save_results_to_file(results2, \"quantum_cryptography_comprehensive.md\")\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "#     # ç¤ºä¾‹3ï¼šæ ‡å‡†ç ”ç©¶\n",
    "#     print(\"ğŸš€ ç¤ºä¾‹3ï¼šæ ‡å‡†ç ”ç©¶\")\n",
    "#     results3 = run_research_assistant(\n",
    "#         topic=\"å¯æŒç»­å‘å±•ä¸ç»¿è‰²èƒ½æºæŠ€æœ¯\",\n",
    "#         research_depth=\"standard\"\n",
    "#     )\n",
    "#     save_results_to_file(results3, \"sustainable_green_energy_standard.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
